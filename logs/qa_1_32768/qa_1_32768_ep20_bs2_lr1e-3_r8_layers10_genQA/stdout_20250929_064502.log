Using data file: needles/32768/qa_1_32768.json
Auto-generated QA directory: outputs/qa_1_32768
Loading data...
Successfully loaded 30 items from JSON file
Loaded 11 generated QA snippets from outputs/qa_1_32768
Loaded 30 examples
Creating model...
Trainable parameters: 21,823,488
Total parameters: 4,739,675,136
Trainable %: 0.46%
Starting training...
Total training steps: 20
==================================================

--- Starting Epoch 1 ---
Step 1 (Epoch 1): Training Loss = 9.4336
{'loss': 9.4336, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.53}
Step 2: Loss = 9.4336, LR = 0.00e+00
Step 2 (Epoch 1): Training Loss = 8.7314
{'loss': 8.7314, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 1.0}

--- Starting Epoch 2 ---
Step 3: Loss = 8.7314, LR = 2.00e-05
Step 3 (Epoch 2): Training Loss = 8.3263
{'loss': 8.3263, 'grad_norm': 19.032772064208984, 'learning_rate': 4e-05, 'epoch': 1.53}
Step 4: Loss = 8.3263, LR = 4.00e-05
Step 4 (Epoch 2): Training Loss = 8.9793
{'loss': 8.9793, 'grad_norm': 23.967742919921875, 'learning_rate': 6e-05, 'epoch': 2.0}

--- Starting Epoch 3 ---
Step 5: Loss = 8.9793, LR = 6.00e-05
Step 5 (Epoch 3): Training Loss = 4.8544
{'loss': 4.8544, 'grad_norm': 12.833636283874512, 'learning_rate': 8e-05, 'epoch': 2.53}
Step 6: Loss = 4.8544, LR = 8.00e-05
Step 6 (Epoch 3): Training Loss = 4.4852
{'loss': 4.4852, 'grad_norm': 19.823972702026367, 'learning_rate': 0.0001, 'epoch': 3.0}

--- Starting Epoch 4 ---
Step 7: Loss = 4.4852, LR = 1.00e-04
Step 7 (Epoch 4): Training Loss = 2.6565
{'loss': 2.6565, 'grad_norm': 16.432754516601562, 'learning_rate': 0.00012, 'epoch': 3.53}
Step 8: Loss = 2.6565, LR = 1.20e-04
Step 8 (Epoch 4): Training Loss = 1.4764
{'loss': 1.4764, 'grad_norm': 1.7201281785964966, 'learning_rate': 0.00014000000000000001, 'epoch': 4.0}

--- Starting Epoch 5 ---
Step 9: Loss = 1.4764, LR = 1.40e-04
Step 9 (Epoch 5): Training Loss = 1.6779
{'loss': 1.6779, 'grad_norm': 0.773787260055542, 'learning_rate': 0.00016, 'epoch': 4.53}
Step 10: Loss = 1.6779, LR = 1.60e-04
Step 10 (Epoch 5): Training Loss = 1.2885
{'loss': 1.2885, 'grad_norm': 0.6805281639099121, 'learning_rate': 0.00017999999999999998, 'epoch': 5.0}

--- Starting Epoch 6 ---
Step 11: Loss = 1.2885, LR = 1.80e-04
Step 11 (Epoch 6): Training Loss = 1.4747
{'loss': 1.4747, 'grad_norm': 0.5188117623329163, 'learning_rate': 0.0002, 'epoch': 5.53}
Step 12: Loss = 1.4747, LR = 2.00e-04
Step 12 (Epoch 6): Training Loss = 1.2111
{'loss': 1.2111, 'grad_norm': 0.2618494927883148, 'learning_rate': 0.00022, 'epoch': 6.0}

--- Starting Epoch 7 ---
Step 13: Loss = 1.2111, LR = 2.20e-04
Step 13 (Epoch 7): Training Loss = 1.3821
{'loss': 1.3821, 'grad_norm': 0.39204901456832886, 'learning_rate': 0.00024, 'epoch': 6.53}
Step 14: Loss = 1.3821, LR = 2.40e-04
Step 14 (Epoch 7): Training Loss = 1.1506
{'loss': 1.1506, 'grad_norm': 0.24609658122062683, 'learning_rate': 0.00026000000000000003, 'epoch': 7.0}

--- Starting Epoch 8 ---
Step 15: Loss = 1.1506, LR = 2.60e-04
Step 15 (Epoch 8): Training Loss = 1.1295
{'loss': 1.1295, 'grad_norm': 0.2612663209438324, 'learning_rate': 0.00028000000000000003, 'epoch': 7.53}
Step 16: Loss = 1.1295, LR = 2.80e-04
Step 16 (Epoch 8): Training Loss = 1.5307
{'loss': 1.5307, 'grad_norm': 0.3481738269329071, 'learning_rate': 0.0003, 'epoch': 8.0}

--- Starting Epoch 9 ---
Step 17: Loss = 1.5307, LR = 3.00e-04
Step 17 (Epoch 9): Training Loss = 1.4991
{'loss': 1.4991, 'grad_norm': 0.23262131214141846, 'learning_rate': 0.00032, 'epoch': 8.53}
Step 18: Loss = 1.4991, LR = 3.20e-04
Step 18 (Epoch 9): Training Loss = 1.5312
{'loss': 1.5312, 'grad_norm': 0.42934367060661316, 'learning_rate': 0.00034, 'epoch': 9.0}

--- Starting Epoch 10 ---
Step 19: Loss = 1.5312, LR = 3.40e-04
Step 19 (Epoch 10): Training Loss = 1.2120
{'loss': 1.212, 'grad_norm': 0.3287893235683441, 'learning_rate': 0.00035999999999999997, 'epoch': 9.53}
Step 20: Loss = 1.2120, LR = 3.60e-04
Step 20 (Epoch 10): Training Loss = 1.7560
{'loss': 1.756, 'grad_norm': 0.4701458215713501, 'learning_rate': 0.00038, 'epoch': 10.0}

--- Starting Epoch 11 ---
Step 21: Loss = 1.7560, LR = 3.80e-04
Step 21 (Epoch 11): Training Loss = 1.3758
{'loss': 1.3758, 'grad_norm': 0.39471906423568726, 'learning_rate': 0.0004, 'epoch': 10.53}
Step 22: Loss = 1.3758, LR = 4.00e-04
Step 22 (Epoch 11): Training Loss = 1.4098
{'loss': 1.4098, 'grad_norm': 0.3565869629383087, 'learning_rate': 0.00042, 'epoch': 11.0}

--- Starting Epoch 12 ---
Step 23: Loss = 1.4098, LR = 4.20e-04
Step 23 (Epoch 12): Training Loss = 0.9533
{'loss': 0.9533, 'grad_norm': 0.18616634607315063, 'learning_rate': 0.00044, 'epoch': 11.53}
Step 24: Loss = 0.9533, LR = 4.40e-04
Step 24 (Epoch 12): Training Loss = 1.0184
{'loss': 1.0184, 'grad_norm': 0.17850273847579956, 'learning_rate': 0.00046, 'epoch': 12.0}

--- Starting Epoch 13 ---
Step 25: Loss = 1.0184, LR = 4.60e-04
Step 25 (Epoch 13): Training Loss = 1.2261
{'loss': 1.2261, 'grad_norm': 0.18186692893505096, 'learning_rate': 0.00048, 'epoch': 12.53}
Step 26: Loss = 1.2261, LR = 4.80e-04
Step 26 (Epoch 13): Training Loss = 1.3173
{'loss': 1.3173, 'grad_norm': 0.3077346980571747, 'learning_rate': 0.0005, 'epoch': 13.0}

--- Starting Epoch 14 ---
Step 27: Loss = 1.3173, LR = 5.00e-04
Step 27 (Epoch 14): Training Loss = 1.0013
{'loss': 1.0013, 'grad_norm': 0.21192814409732819, 'learning_rate': 0.0005200000000000001, 'epoch': 13.53}
Step 28: Loss = 1.0013, LR = 5.20e-04
Step 28 (Epoch 14): Training Loss = 1.0963
{'loss': 1.0963, 'grad_norm': 0.22020450234413147, 'learning_rate': 0.00054, 'epoch': 14.0}

--- Starting Epoch 15 ---
Step 29: Loss = 1.0963, LR = 5.40e-04
Step 29 (Epoch 15): Training Loss = 1.2485
{'loss': 1.2485, 'grad_norm': 0.22811290621757507, 'learning_rate': 0.0005600000000000001, 'epoch': 14.53}
Step 30: Loss = 1.2485, LR = 5.60e-04
Step 30 (Epoch 15): Training Loss = 0.7455
{'loss': 0.7455, 'grad_norm': 0.3174169361591339, 'learning_rate': 0.00058, 'epoch': 15.0}

--- Starting Epoch 16 ---
Step 31: Loss = 0.7455, LR = 5.80e-04
Step 31 (Epoch 16): Training Loss = 1.3692
{'loss': 1.3692, 'grad_norm': 0.3671133816242218, 'learning_rate': 0.0006, 'epoch': 15.53}
Step 32: Loss = 1.3692, LR = 6.00e-04
Step 32 (Epoch 16): Training Loss = 0.8255
{'loss': 0.8255, 'grad_norm': 0.3142991065979004, 'learning_rate': 0.00062, 'epoch': 16.0}

--- Starting Epoch 17 ---
Step 33: Loss = 0.8255, LR = 6.20e-04
Step 33 (Epoch 17): Training Loss = 0.9177
{'loss': 0.9177, 'grad_norm': 0.3116239607334137, 'learning_rate': 0.00064, 'epoch': 16.53}
Step 34: Loss = 0.9177, LR = 6.40e-04
Step 34 (Epoch 17): Training Loss = 0.5040
{'loss': 0.504, 'grad_norm': 0.3328178822994232, 'learning_rate': 0.00066, 'epoch': 17.0}

--- Starting Epoch 18 ---
Step 35: Loss = 0.5040, LR = 6.60e-04
Step 35 (Epoch 18): Training Loss = 0.7163
{'loss': 0.7163, 'grad_norm': 0.42208337783813477, 'learning_rate': 0.00068, 'epoch': 17.53}
Step 36: Loss = 0.7163, LR = 6.80e-04
Step 36 (Epoch 18): Training Loss = 0.6172
{'loss': 0.6172, 'grad_norm': 0.47802093625068665, 'learning_rate': 0.0007, 'epoch': 18.0}

--- Starting Epoch 19 ---
Step 37: Loss = 0.6172, LR = 7.00e-04
Step 37 (Epoch 19): Training Loss = 0.3335
{'loss': 0.3335, 'grad_norm': 0.3548063039779663, 'learning_rate': 0.0007199999999999999, 'epoch': 18.53}
Step 38: Loss = 0.3335, LR = 7.20e-04
Step 38 (Epoch 19): Training Loss = 0.3859
{'loss': 0.3859, 'grad_norm': 0.5865421891212463, 'learning_rate': 0.00074, 'epoch': 19.0}

--- Starting Epoch 20 ---
Step 39: Loss = 0.3859, LR = 7.40e-04
Step 39 (Epoch 20): Training Loss = 0.2548
{'loss': 0.2548, 'grad_norm': 0.4743175506591797, 'learning_rate': 0.00076, 'epoch': 19.53}
Step 40: Loss = 0.2548, LR = 7.60e-04
Step 40 (Epoch 20): Training Loss = 0.2593
{'loss': 0.2593, 'grad_norm': 0.5498605370521545, 'learning_rate': 0.0007800000000000001, 'epoch': 20.0}
{'train_runtime': 703.4027, 'train_samples_per_second': 0.853, 'train_steps_per_second': 0.057, 'train_loss': 2.0840490467846395, 'epoch': 20.0}
==================================================
Saving adapter...
Training complete! Adapter saved to /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_genQA
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_genQA
needle_size: 32768
needle_type: qa_1
max_samples: 100
output_file: benchmark_results.json
benchmark: all
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 2
Using data file: needles/32768/qa_1_32768.json
Loading model...

=== Running QA Benchmark ===
Loading test data...
Loading QA data from needles/32768/qa_1_32768.json
Successfully loaded 30 items from JSON file
Evaluating on 30 samples
Generating predictions with batch size 2...

--- Sample Results ---

Sample 1:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: In what country is Normandy located?
Expected: France
Predicted: 

Sample 2:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: When were the Normans in Normandy?
Expected: 10th and 11th centuries
Predicted: Cite error: There are 2 missing citations. The list of references is empty

Sample 3:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: From which countries did the Norse originate?
Expected: Denmark, Iceland and Norway
Predicted: Oxlow, Baden-Powell, and Hulton
Evaluating...

=== QA Benchmark Results ===
Exact Accuracy: 0.200
Partial Accuracy: 0.300
Exact Matches: 6/30
Partial Matches: 3/30

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) â€” falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...
Perplexity: 31.560
Average Loss: 3.452
Total Tokens: 11644

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to benchmark_results.json
Starting LoRA training with settings...
Needle size: 32768, Needle type: qa_1
Epochs: 20, Batch size: 2, Learning rate: 0.001
Command: python train_lora_optimized.py --model_name Qwen/Qwen3-8B --needle_size 32768 --needle_type qa_1 --epochs 20 --batch_size 2 --learning_rate 0.001 --adapter_r 8 --adapter_layers 10 --output_dir /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_genQA --use_generated_qa

Training completed successfully!
