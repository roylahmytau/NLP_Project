Using data file: needles/2048/niah_single_2_2048.json
Loading data...
Successfully loaded 100 items from JSON file
Loaded 100 examples
Creating model...
Trainable parameters: 21,823,488
Total parameters: 4,739,675,136
Trainable %: 0.46%
Starting training...
Total training steps: 120
==================================================

--- Starting Epoch 1 ---
Step 1 (Epoch 1): Training Loss = 5.3470
{'loss': 5.347, 'grad_norm': 3.456225872039795, 'learning_rate': 0.0, 'epoch': 0.16}
Step 2: Loss = 5.3470, LR = 0.00e+00
Step 2 (Epoch 1): Training Loss = 5.4525
{'loss': 5.4525, 'grad_norm': 3.523566961288452, 'learning_rate': 2e-05, 'epoch': 0.32}
Step 3: Loss = 5.4525, LR = 2.00e-05
Step 3 (Epoch 1): Training Loss = 5.3420
{'loss': 5.342, 'grad_norm': 3.463796377182007, 'learning_rate': 4e-05, 'epoch': 0.48}
Step 4: Loss = 5.3420, LR = 4.00e-05
Step 4 (Epoch 1): Training Loss = 5.1917
{'loss': 5.1917, 'grad_norm': 3.2917838096618652, 'learning_rate': 6e-05, 'epoch': 0.64}
Step 5: Loss = 5.1917, LR = 6.00e-05
Step 5 (Epoch 1): Training Loss = 4.9525
{'loss': 4.9525, 'grad_norm': 3.464139699935913, 'learning_rate': 8e-05, 'epoch': 0.8}
Step 6: Loss = 4.9525, LR = 8.00e-05
Step 6 (Epoch 1): Training Loss = 4.6152
{'loss': 4.6152, 'grad_norm': 3.6570611000061035, 'learning_rate': 0.0001, 'epoch': 0.96}
Step 7: Loss = 4.6152, LR = 1.00e-04
Step 7 (Epoch 1): Training Loss = 4.0153
{'loss': 4.0153, 'grad_norm': 3.3971900939941406, 'learning_rate': 0.00012, 'epoch': 1.0}

--- Starting Epoch 2 ---
Step 8: Loss = 4.0153, LR = 1.20e-04
Step 8 (Epoch 2): Training Loss = 3.7002
{'loss': 3.7002, 'grad_norm': 3.1458611488342285, 'learning_rate': 0.00014000000000000001, 'epoch': 1.16}
Step 9: Loss = 3.7002, LR = 1.40e-04
Step 9 (Epoch 2): Training Loss = 3.4095
{'loss': 3.4095, 'grad_norm': 3.037379503250122, 'learning_rate': 0.00016, 'epoch': 1.32}
Step 10: Loss = 3.4095, LR = 1.60e-04
Step 10 (Epoch 2): Training Loss = 3.0550
{'loss': 3.055, 'grad_norm': 3.153440237045288, 'learning_rate': 0.00017999999999999998, 'epoch': 1.48}
Step 11: Loss = 3.0550, LR = 1.80e-04
Step 11 (Epoch 2): Training Loss = 2.6881
{'loss': 2.6881, 'grad_norm': 2.3507161140441895, 'learning_rate': 0.0002, 'epoch': 1.64}
Step 12: Loss = 2.6881, LR = 2.00e-04
Step 12 (Epoch 2): Training Loss = 2.3685
{'loss': 2.3685, 'grad_norm': 2.0992281436920166, 'learning_rate': 0.00022, 'epoch': 1.8}
Step 13: Loss = 2.3685, LR = 2.20e-04
Step 13 (Epoch 2): Training Loss = 2.0540
{'loss': 2.054, 'grad_norm': 1.8355836868286133, 'learning_rate': 0.00024, 'epoch': 1.96}
Step 14: Loss = 2.0540, LR = 2.40e-04
Step 14 (Epoch 2): Training Loss = 1.7907
{'loss': 1.7907, 'grad_norm': 2.0675716400146484, 'learning_rate': 0.00026000000000000003, 'epoch': 2.0}

--- Starting Epoch 3 ---
Step 15: Loss = 1.7907, LR = 2.60e-04
Step 15 (Epoch 3): Training Loss = 1.3985
{'loss': 1.3985, 'grad_norm': 3.581069231033325, 'learning_rate': 0.00028000000000000003, 'epoch': 2.16}
Step 16: Loss = 1.3985, LR = 2.80e-04
Step 16 (Epoch 3): Training Loss = 1.1651
{'loss': 1.1651, 'grad_norm': 6.348697185516357, 'learning_rate': 0.0003, 'epoch': 2.32}
Step 17: Loss = 1.1651, LR = 3.00e-04
Step 17 (Epoch 3): Training Loss = 1.0969
{'loss': 1.0969, 'grad_norm': 5.019772052764893, 'learning_rate': 0.00032, 'epoch': 2.48}
Step 18: Loss = 1.0969, LR = 3.20e-04
Step 18 (Epoch 3): Training Loss = 0.9908
{'loss': 0.9908, 'grad_norm': 19.269216537475586, 'learning_rate': 0.00034, 'epoch': 2.64}
Step 19: Loss = 0.9908, LR = 3.40e-04
Step 19 (Epoch 3): Training Loss = 0.8906
{'loss': 0.8906, 'grad_norm': 3.0882601737976074, 'learning_rate': 0.00035999999999999997, 'epoch': 2.8}
Step 20: Loss = 0.8906, LR = 3.60e-04
Step 20 (Epoch 3): Training Loss = 0.8271
{'loss': 0.8271, 'grad_norm': 2.301703929901123, 'learning_rate': 0.00038, 'epoch': 2.96}
Step 21: Loss = 0.8271, LR = 3.80e-04
Step 21 (Epoch 3): Training Loss = 0.7985
{'loss': 0.7985, 'grad_norm': 3.557565689086914, 'learning_rate': 0.0004, 'epoch': 3.0}

--- Starting Epoch 4 ---
Step 22: Loss = 0.7985, LR = 4.00e-04
Step 22 (Epoch 4): Training Loss = 0.8323
{'loss': 0.8323, 'grad_norm': 3.984900951385498, 'learning_rate': 0.00042, 'epoch': 3.16}
Step 23: Loss = 0.8323, LR = 4.20e-04
Step 23 (Epoch 4): Training Loss = 0.7401
{'loss': 0.7401, 'grad_norm': 0.9316248893737793, 'learning_rate': 0.00044, 'epoch': 3.32}
Step 24: Loss = 0.7401, LR = 4.40e-04
Step 24 (Epoch 4): Training Loss = 0.7015
{'loss': 0.7015, 'grad_norm': 1.2587252855300903, 'learning_rate': 0.00046, 'epoch': 3.48}
Step 25: Loss = 0.7015, LR = 4.60e-04
Step 25 (Epoch 4): Training Loss = 0.7283
{'loss': 0.7283, 'grad_norm': 11.212777137756348, 'learning_rate': 0.00048, 'epoch': 3.64}
Step 26: Loss = 0.7283, LR = 4.80e-04
Step 26 (Epoch 4): Training Loss = 0.6622
{'loss': 0.6622, 'grad_norm': 1.0361764430999756, 'learning_rate': 0.0005, 'epoch': 3.8}
Step 27: Loss = 0.6622, LR = 5.00e-04
Step 27 (Epoch 4): Training Loss = 0.6502
{'loss': 0.6502, 'grad_norm': 1.1605366468429565, 'learning_rate': 0.0005200000000000001, 'epoch': 3.96}
Step 28: Loss = 0.6502, LR = 5.20e-04
Step 28 (Epoch 4): Training Loss = 0.5957
{'loss': 0.5957, 'grad_norm': 1.3975399732589722, 'learning_rate': 0.00054, 'epoch': 4.0}

--- Starting Epoch 5 ---
Step 29: Loss = 0.5957, LR = 5.40e-04
Step 29 (Epoch 5): Training Loss = 0.6124
{'loss': 0.6124, 'grad_norm': 1.1222604513168335, 'learning_rate': 0.0005600000000000001, 'epoch': 4.16}
Step 30: Loss = 0.6124, LR = 5.60e-04
Step 30 (Epoch 5): Training Loss = 0.5490
{'loss': 0.549, 'grad_norm': 0.3901805877685547, 'learning_rate': 0.00058, 'epoch': 4.32}
Step 31: Loss = 0.5490, LR = 5.80e-04
Step 31 (Epoch 5): Training Loss = 0.5531
{'loss': 0.5531, 'grad_norm': 0.24417972564697266, 'learning_rate': 0.0006, 'epoch': 4.48}
Step 32: Loss = 0.5531, LR = 6.00e-04
Step 32 (Epoch 5): Training Loss = 0.5696
{'loss': 0.5696, 'grad_norm': 0.285535603761673, 'learning_rate': 0.00062, 'epoch': 4.64}
Step 33: Loss = 0.5696, LR = 6.20e-04
Step 33 (Epoch 5): Training Loss = 0.5389
{'loss': 0.5389, 'grad_norm': 0.47072988748550415, 'learning_rate': 0.00064, 'epoch': 4.8}
Step 34: Loss = 0.5389, LR = 6.40e-04
Step 34 (Epoch 5): Training Loss = 0.5452
{'loss': 0.5452, 'grad_norm': 0.2015102505683899, 'learning_rate': 0.00066, 'epoch': 4.96}
Step 35: Loss = 0.5452, LR = 6.60e-04
Step 35 (Epoch 5): Training Loss = 0.5523
{'loss': 0.5523, 'grad_norm': 0.45725417137145996, 'learning_rate': 0.00068, 'epoch': 5.0}

--- Starting Epoch 6 ---
Step 36: Loss = 0.5523, LR = 6.80e-04
Step 36 (Epoch 6): Training Loss = 0.5173
{'loss': 0.5173, 'grad_norm': 0.2118207812309265, 'learning_rate': 0.0007, 'epoch': 5.16}
Step 37: Loss = 0.5173, LR = 7.00e-04
Step 37 (Epoch 6): Training Loss = 0.4868
{'loss': 0.4868, 'grad_norm': 0.22638605535030365, 'learning_rate': 0.0007199999999999999, 'epoch': 5.32}
Step 38: Loss = 0.4868, LR = 7.20e-04
Step 38 (Epoch 6): Training Loss = 0.5109
{'loss': 0.5109, 'grad_norm': 0.2750498354434967, 'learning_rate': 0.00074, 'epoch': 5.48}
Step 39: Loss = 0.5109, LR = 7.40e-04
Step 39 (Epoch 6): Training Loss = 0.4851
{'loss': 0.4851, 'grad_norm': 0.28401610255241394, 'learning_rate': 0.00076, 'epoch': 5.64}
Step 40: Loss = 0.4851, LR = 7.60e-04
Step 40 (Epoch 6): Training Loss = 0.5181
{'loss': 0.5181, 'grad_norm': 0.3996988534927368, 'learning_rate': 0.0007800000000000001, 'epoch': 5.8}
Step 41: Loss = 0.5181, LR = 7.80e-04
Step 41 (Epoch 6): Training Loss = 0.5009
{'loss': 0.5009, 'grad_norm': 0.3078139126300812, 'learning_rate': 0.0008, 'epoch': 5.96}
Step 42: Loss = 0.5009, LR = 8.00e-04
Step 42 (Epoch 6): Training Loss = 0.4687
{'loss': 0.4687, 'grad_norm': 0.5362439751625061, 'learning_rate': 0.00082, 'epoch': 6.0}

--- Starting Epoch 7 ---
Step 43: Loss = 0.4687, LR = 8.20e-04
Step 43 (Epoch 7): Training Loss = 0.4456
{'loss': 0.4456, 'grad_norm': 0.2854042053222656, 'learning_rate': 0.00084, 'epoch': 6.16}
Step 44: Loss = 0.4456, LR = 8.40e-04
Step 44 (Epoch 7): Training Loss = 0.4444
{'loss': 0.4444, 'grad_norm': 0.2803616225719452, 'learning_rate': 0.00086, 'epoch': 6.32}
Step 45: Loss = 0.4444, LR = 8.60e-04
Step 45 (Epoch 7): Training Loss = 0.4629
{'loss': 0.4629, 'grad_norm': 0.2973325550556183, 'learning_rate': 0.00088, 'epoch': 6.48}
Step 46: Loss = 0.4629, LR = 8.80e-04
Step 46 (Epoch 7): Training Loss = 0.4748
{'loss': 0.4748, 'grad_norm': 0.4579862952232361, 'learning_rate': 0.0009000000000000001, 'epoch': 6.64}
Step 47: Loss = 0.4748, LR = 9.00e-04
Step 47 (Epoch 7): Training Loss = 0.4305
{'loss': 0.4305, 'grad_norm': 0.2643326222896576, 'learning_rate': 0.00092, 'epoch': 6.8}
Step 48: Loss = 0.4305, LR = 9.20e-04
Step 48 (Epoch 7): Training Loss = 0.4886
{'loss': 0.4886, 'grad_norm': 0.3916234076023102, 'learning_rate': 0.00094, 'epoch': 6.96}
Step 49: Loss = 0.4886, LR = 9.40e-04
Step 49 (Epoch 7): Training Loss = 0.4330
{'loss': 0.433, 'grad_norm': 0.5576673746109009, 'learning_rate': 0.00096, 'epoch': 7.0}

--- Starting Epoch 8 ---
Step 50: Loss = 0.4330, LR = 9.60e-04
Step 50 (Epoch 8): Training Loss = 0.4254
{'loss': 0.4254, 'grad_norm': 0.2524700164794922, 'learning_rate': 0.00098, 'epoch': 7.16}
Step 51: Loss = 0.4254, LR = 9.80e-04
Step 51 (Epoch 8): Training Loss = 0.4055
{'loss': 0.4055, 'grad_norm': 0.26840850710868835, 'learning_rate': 0.001, 'epoch': 7.32}
Step 52: Loss = 0.4055, LR = 1.00e-03
Step 52 (Epoch 8): Training Loss = 0.4270
{'loss': 0.427, 'grad_norm': 0.31119903922080994, 'learning_rate': 0.000988888888888889, 'epoch': 7.48}
Step 53: Loss = 0.4270, LR = 9.89e-04
Step 53 (Epoch 8): Training Loss = 0.4112
{'loss': 0.4112, 'grad_norm': 0.35897573828697205, 'learning_rate': 0.0009777777777777777, 'epoch': 7.64}
Step 54: Loss = 0.4112, LR = 9.78e-04
Step 54 (Epoch 8): Training Loss = 0.4264
{'loss': 0.4264, 'grad_norm': 0.43491995334625244, 'learning_rate': 0.0009666666666666667, 'epoch': 7.8}
Step 55: Loss = 0.4264, LR = 9.67e-04
Step 55 (Epoch 8): Training Loss = 0.3885
{'loss': 0.3885, 'grad_norm': 0.34512537717819214, 'learning_rate': 0.0009555555555555556, 'epoch': 7.96}
Step 56: Loss = 0.3885, LR = 9.56e-04
Step 56 (Epoch 8): Training Loss = 0.4665
{'loss': 0.4665, 'grad_norm': 0.643802285194397, 'learning_rate': 0.0009444444444444445, 'epoch': 8.0}

--- Starting Epoch 9 ---
Step 57: Loss = 0.4665, LR = 9.44e-04
Step 57 (Epoch 9): Training Loss = 0.3731
{'loss': 0.3731, 'grad_norm': 0.5880924463272095, 'learning_rate': 0.0009333333333333333, 'epoch': 8.16}
Step 58: Loss = 0.3731, LR = 9.33e-04
Step 58 (Epoch 9): Training Loss = 0.3532
{'loss': 0.3532, 'grad_norm': 0.35034650564193726, 'learning_rate': 0.0009222222222222223, 'epoch': 8.32}
Step 59: Loss = 0.3532, LR = 9.22e-04
Step 59 (Epoch 9): Training Loss = 0.3901
{'loss': 0.3901, 'grad_norm': 1.8224798440933228, 'learning_rate': 0.0009111111111111111, 'epoch': 8.48}
Step 60: Loss = 0.3901, LR = 9.11e-04
Step 60 (Epoch 9): Training Loss = 0.3634
{'loss': 0.3634, 'grad_norm': 0.38593900203704834, 'learning_rate': 0.0009000000000000001, 'epoch': 8.64}
Step 61: Loss = 0.3634, LR = 9.00e-04
Step 61 (Epoch 9): Training Loss = 0.3783
{'loss': 0.3783, 'grad_norm': 0.6994383931159973, 'learning_rate': 0.0008888888888888888, 'epoch': 8.8}
Step 62: Loss = 0.3783, LR = 8.89e-04
Step 62 (Epoch 9): Training Loss = 0.3494
{'loss': 0.3494, 'grad_norm': 0.37611526250839233, 'learning_rate': 0.0008777777777777778, 'epoch': 8.96}
Step 63: Loss = 0.3494, LR = 8.78e-04
Step 63 (Epoch 9): Training Loss = 0.3857
{'loss': 0.3857, 'grad_norm': 0.6564540266990662, 'learning_rate': 0.0008666666666666667, 'epoch': 9.0}

--- Starting Epoch 10 ---
Step 64: Loss = 0.3857, LR = 8.67e-04
Step 64 (Epoch 10): Training Loss = 0.3207
{'loss': 0.3207, 'grad_norm': 0.2977166771888733, 'learning_rate': 0.0008555555555555556, 'epoch': 9.16}
Step 65: Loss = 0.3207, LR = 8.56e-04
Step 65 (Epoch 10): Training Loss = 0.3238
{'loss': 0.3238, 'grad_norm': 0.3946078419685364, 'learning_rate': 0.0008444444444444444, 'epoch': 9.32}
Step 66: Loss = 0.3238, LR = 8.44e-04
Step 66 (Epoch 10): Training Loss = 0.3217
{'loss': 0.3217, 'grad_norm': 0.5293453931808472, 'learning_rate': 0.0008333333333333334, 'epoch': 9.48}
Step 67: Loss = 0.3217, LR = 8.33e-04
Step 67 (Epoch 10): Training Loss = 0.3115
{'loss': 0.3115, 'grad_norm': 0.5862857103347778, 'learning_rate': 0.0008222222222222222, 'epoch': 9.64}
Step 68: Loss = 0.3115, LR = 8.22e-04
Step 68 (Epoch 10): Training Loss = 0.3197
{'loss': 0.3197, 'grad_norm': 0.5302373766899109, 'learning_rate': 0.0008111111111111111, 'epoch': 9.8}
Step 69: Loss = 0.3197, LR = 8.11e-04
Step 69 (Epoch 10): Training Loss = 0.3338
{'loss': 0.3338, 'grad_norm': 0.5933747887611389, 'learning_rate': 0.0008, 'epoch': 9.96}
Step 70: Loss = 0.3338, LR = 8.00e-04
Step 70 (Epoch 10): Training Loss = 0.3555
{'loss': 0.3555, 'grad_norm': 0.7976886034011841, 'learning_rate': 0.0007888888888888889, 'epoch': 10.0}

--- Starting Epoch 11 ---
Step 71: Loss = 0.3555, LR = 7.89e-04
Step 71 (Epoch 11): Training Loss = 0.3112
{'loss': 0.3112, 'grad_norm': 0.3574353754520416, 'learning_rate': 0.0007777777777777778, 'epoch': 10.16}
Step 72: Loss = 0.3112, LR = 7.78e-04
Step 72 (Epoch 11): Training Loss = 0.2983
{'loss': 0.2983, 'grad_norm': 0.42251986265182495, 'learning_rate': 0.0007666666666666667, 'epoch': 10.32}
Step 73: Loss = 0.2983, LR = 7.67e-04
Step 73 (Epoch 11): Training Loss = 0.2943
{'loss': 0.2943, 'grad_norm': 0.34704867005348206, 'learning_rate': 0.0007555555555555555, 'epoch': 10.48}
Step 74: Loss = 0.2943, LR = 7.56e-04
Step 74 (Epoch 11): Training Loss = 0.2760
{'loss': 0.276, 'grad_norm': 0.41737762093544006, 'learning_rate': 0.0007444444444444445, 'epoch': 10.64}
Step 75: Loss = 0.2760, LR = 7.44e-04
Step 75 (Epoch 11): Training Loss = 0.3205
{'loss': 0.3205, 'grad_norm': 0.6768856644630432, 'learning_rate': 0.0007333333333333333, 'epoch': 10.8}
Step 76: Loss = 0.3205, LR = 7.33e-04
Step 76 (Epoch 11): Training Loss = 0.2847
{'loss': 0.2847, 'grad_norm': 0.42744120955467224, 'learning_rate': 0.0007222222222222222, 'epoch': 10.96}
Step 77: Loss = 0.2847, LR = 7.22e-04
Step 77 (Epoch 11): Training Loss = 0.2753
{'loss': 0.2753, 'grad_norm': 0.7415306568145752, 'learning_rate': 0.0007111111111111111, 'epoch': 11.0}

--- Starting Epoch 12 ---
Step 78: Loss = 0.2753, LR = 7.11e-04
Step 78 (Epoch 12): Training Loss = 0.2567
{'loss': 0.2567, 'grad_norm': 0.4776187539100647, 'learning_rate': 0.0007, 'epoch': 11.16}
Step 79: Loss = 0.2567, LR = 7.00e-04
Step 79 (Epoch 12): Training Loss = 0.2400
{'loss': 0.24, 'grad_norm': 0.521166205406189, 'learning_rate': 0.000688888888888889, 'epoch': 11.32}
Step 80: Loss = 0.2400, LR = 6.89e-04
Step 80 (Epoch 12): Training Loss = 0.2762
{'loss': 0.2762, 'grad_norm': 0.6552595496177673, 'learning_rate': 0.0006777777777777778, 'epoch': 11.48}
Step 81: Loss = 0.2762, LR = 6.78e-04
Step 81 (Epoch 12): Training Loss = 0.2759
{'loss': 0.2759, 'grad_norm': 0.6220254898071289, 'learning_rate': 0.0006666666666666666, 'epoch': 11.64}
Step 82: Loss = 0.2759, LR = 6.67e-04
Step 82 (Epoch 12): Training Loss = 0.2311
{'loss': 0.2311, 'grad_norm': 0.5702441334724426, 'learning_rate': 0.0006555555555555556, 'epoch': 11.8}
Step 83: Loss = 0.2311, LR = 6.56e-04
Step 83 (Epoch 12): Training Loss = 0.2812
{'loss': 0.2812, 'grad_norm': 0.5903293490409851, 'learning_rate': 0.0006444444444444444, 'epoch': 11.96}
Step 84: Loss = 0.2812, LR = 6.44e-04
Step 84 (Epoch 12): Training Loss = 0.2491
{'loss': 0.2491, 'grad_norm': 0.8441481590270996, 'learning_rate': 0.0006333333333333333, 'epoch': 12.0}

--- Starting Epoch 13 ---
Step 85: Loss = 0.2491, LR = 6.33e-04
Step 85 (Epoch 13): Training Loss = 0.2079
{'loss': 0.2079, 'grad_norm': 3.8127973079681396, 'learning_rate': 0.0006222222222222223, 'epoch': 12.16}
Step 86: Loss = 0.2079, LR = 6.22e-04
Step 86 (Epoch 13): Training Loss = 0.2205
{'loss': 0.2205, 'grad_norm': 0.43467462062835693, 'learning_rate': 0.0006111111111111112, 'epoch': 12.32}
Step 87: Loss = 0.2205, LR = 6.11e-04
Step 87 (Epoch 13): Training Loss = 0.2257
{'loss': 0.2257, 'grad_norm': 0.49834808707237244, 'learning_rate': 0.0006, 'epoch': 12.48}
Step 88: Loss = 0.2257, LR = 6.00e-04
Step 88 (Epoch 13): Training Loss = 0.2178
{'loss': 0.2178, 'grad_norm': 0.5534918904304504, 'learning_rate': 0.0005888888888888889, 'epoch': 12.64}
Step 89: Loss = 0.2178, LR = 5.89e-04
Step 89 (Epoch 13): Training Loss = 0.2201
{'loss': 0.2201, 'grad_norm': 0.5125772356987, 'learning_rate': 0.0005777777777777778, 'epoch': 12.8}
Step 90: Loss = 0.2201, LR = 5.78e-04
Step 90 (Epoch 13): Training Loss = 0.2327
{'loss': 0.2327, 'grad_norm': 0.6050131916999817, 'learning_rate': 0.0005666666666666667, 'epoch': 12.96}
Step 91: Loss = 0.2327, LR = 5.67e-04
Step 91 (Epoch 13): Training Loss = 0.2265
{'loss': 0.2265, 'grad_norm': 0.7256610989570618, 'learning_rate': 0.0005555555555555556, 'epoch': 13.0}

--- Starting Epoch 14 ---
Step 92: Loss = 0.2265, LR = 5.56e-04
Step 92 (Epoch 14): Training Loss = 0.2087
{'loss': 0.2087, 'grad_norm': 3.4619481563568115, 'learning_rate': 0.0005444444444444444, 'epoch': 13.16}
Step 93: Loss = 0.2087, LR = 5.44e-04
Step 93 (Epoch 14): Training Loss = 0.1807
{'loss': 0.1807, 'grad_norm': 0.45586156845092773, 'learning_rate': 0.0005333333333333334, 'epoch': 13.32}
Step 94: Loss = 0.1807, LR = 5.33e-04
Step 94 (Epoch 14): Training Loss = 0.2007
{'loss': 0.2007, 'grad_norm': 0.59767746925354, 'learning_rate': 0.0005222222222222223, 'epoch': 13.48}
Step 95: Loss = 0.2007, LR = 5.22e-04
Step 95 (Epoch 14): Training Loss = 0.1974
{'loss': 0.1974, 'grad_norm': 1.6302663087844849, 'learning_rate': 0.0005111111111111111, 'epoch': 13.64}
Step 96: Loss = 0.1974, LR = 5.11e-04
Step 96 (Epoch 14): Training Loss = 0.2155
{'loss': 0.2155, 'grad_norm': 1.8929460048675537, 'learning_rate': 0.0005, 'epoch': 13.8}
Step 97: Loss = 0.2155, LR = 5.00e-04
Step 97 (Epoch 14): Training Loss = 0.1966
{'loss': 0.1966, 'grad_norm': 0.6037887930870056, 'learning_rate': 0.0004888888888888889, 'epoch': 13.96}
Step 98: Loss = 0.1966, LR = 4.89e-04
Step 98 (Epoch 14): Training Loss = 0.2048
{'loss': 0.2048, 'grad_norm': 0.8034021258354187, 'learning_rate': 0.0004777777777777778, 'epoch': 14.0}

--- Starting Epoch 15 ---
Step 99: Loss = 0.2048, LR = 4.78e-04
Step 99 (Epoch 15): Training Loss = 0.1574
{'loss': 0.1574, 'grad_norm': 0.30570730566978455, 'learning_rate': 0.00046666666666666666, 'epoch': 14.16}
Step 100: Loss = 0.1574, LR = 4.67e-04
Step 100 (Epoch 15): Training Loss = 0.1631
{'loss': 0.1631, 'grad_norm': 0.29783686995506287, 'learning_rate': 0.00045555555555555556, 'epoch': 14.32}
Step 101: Loss = 0.1631, LR = 4.56e-04
Step 101 (Epoch 15): Training Loss = 0.1691
{'loss': 0.1691, 'grad_norm': 0.5971293449401855, 'learning_rate': 0.0004444444444444444, 'epoch': 14.48}
Step 102: Loss = 0.1691, LR = 4.44e-04
Step 102 (Epoch 15): Training Loss = 0.1664
{'loss': 0.1664, 'grad_norm': 0.4004455804824829, 'learning_rate': 0.00043333333333333337, 'epoch': 14.64}
Step 103: Loss = 0.1664, LR = 4.33e-04
Step 103 (Epoch 15): Training Loss = 0.1703
{'loss': 0.1703, 'grad_norm': 0.39320555329322815, 'learning_rate': 0.0004222222222222222, 'epoch': 14.8}
Step 104: Loss = 0.1703, LR = 4.22e-04
Step 104 (Epoch 15): Training Loss = 0.1616
{'loss': 0.1616, 'grad_norm': 0.36188894510269165, 'learning_rate': 0.0004111111111111111, 'epoch': 14.96}
Step 105: Loss = 0.1616, LR = 4.11e-04
Step 105 (Epoch 15): Training Loss = 0.1605
{'loss': 0.1605, 'grad_norm': 0.5919747948646545, 'learning_rate': 0.0004, 'epoch': 15.0}

--- Starting Epoch 16 ---
Step 106: Loss = 0.1605, LR = 4.00e-04
Step 106 (Epoch 16): Training Loss = 0.1429
{'loss': 0.1429, 'grad_norm': 0.22705084085464478, 'learning_rate': 0.0003888888888888889, 'epoch': 15.16}
Step 107: Loss = 0.1429, LR = 3.89e-04
Step 107 (Epoch 16): Training Loss = 0.1308
{'loss': 0.1308, 'grad_norm': 0.2174399495124817, 'learning_rate': 0.00037777777777777777, 'epoch': 15.32}
Step 108: Loss = 0.1308, LR = 3.78e-04
Step 108 (Epoch 16): Training Loss = 0.1524
{'loss': 0.1524, 'grad_norm': 0.41471514105796814, 'learning_rate': 0.00036666666666666667, 'epoch': 15.48}
Step 109: Loss = 0.1524, LR = 3.67e-04
Step 109 (Epoch 16): Training Loss = 0.1337
{'loss': 0.1337, 'grad_norm': 0.3060705363750458, 'learning_rate': 0.00035555555555555557, 'epoch': 15.64}
Step 110: Loss = 0.1337, LR = 3.56e-04
Step 110 (Epoch 16): Training Loss = 0.1483
{'loss': 0.1483, 'grad_norm': 0.4618357717990875, 'learning_rate': 0.0003444444444444445, 'epoch': 15.8}
Step 111: Loss = 0.1483, LR = 3.44e-04
Step 111 (Epoch 16): Training Loss = 0.1352
{'loss': 0.1352, 'grad_norm': 0.37853196263313293, 'learning_rate': 0.0003333333333333333, 'epoch': 15.96}
Step 112: Loss = 0.1352, LR = 3.33e-04
Step 112 (Epoch 16): Training Loss = 0.1124
{'loss': 0.1124, 'grad_norm': 0.864113986492157, 'learning_rate': 0.0003222222222222222, 'epoch': 16.0}

--- Starting Epoch 17 ---
Step 113: Loss = 0.1124, LR = 3.22e-04
Step 113 (Epoch 17): Training Loss = 0.1197
{'loss': 0.1197, 'grad_norm': 0.2523099184036255, 'learning_rate': 0.0003111111111111111, 'epoch': 16.16}
Step 114: Loss = 0.1197, LR = 3.11e-04
Step 114 (Epoch 17): Training Loss = 0.1185
{'loss': 0.1185, 'grad_norm': 0.1657118946313858, 'learning_rate': 0.0003, 'epoch': 16.32}
Step 115: Loss = 0.1185, LR = 3.00e-04
Step 115 (Epoch 17): Training Loss = 0.1217
{'loss': 0.1217, 'grad_norm': 0.18075315654277802, 'learning_rate': 0.0002888888888888889, 'epoch': 16.48}
Step 116: Loss = 0.1217, LR = 2.89e-04
Step 116 (Epoch 17): Training Loss = 0.1265
{'loss': 0.1265, 'grad_norm': 0.40445104241371155, 'learning_rate': 0.0002777777777777778, 'epoch': 16.64}
Step 117: Loss = 0.1265, LR = 2.78e-04
Step 117 (Epoch 17): Training Loss = 0.1164
{'loss': 0.1164, 'grad_norm': 0.31992781162261963, 'learning_rate': 0.0002666666666666667, 'epoch': 16.8}
Step 118: Loss = 0.1164, LR = 2.67e-04
Step 118 (Epoch 17): Training Loss = 0.1228
{'loss': 0.1228, 'grad_norm': 0.22135178744792938, 'learning_rate': 0.00025555555555555553, 'epoch': 16.96}
Step 119: Loss = 0.1228, LR = 2.56e-04
Step 119 (Epoch 17): Training Loss = 0.1241
{'loss': 0.1241, 'grad_norm': 0.48456311225891113, 'learning_rate': 0.00024444444444444443, 'epoch': 17.0}

--- Starting Epoch 18 ---
Step 120: Loss = 0.1241, LR = 2.44e-04
Step 120 (Epoch 18): Training Loss = 0.1171
{'loss': 0.1171, 'grad_norm': 0.16490814089775085, 'learning_rate': 0.00023333333333333333, 'epoch': 17.16}
Step 121: Loss = 0.1171, LR = 2.33e-04
Step 121 (Epoch 18): Training Loss = 0.1118
{'loss': 0.1118, 'grad_norm': 0.18067358434200287, 'learning_rate': 0.0002222222222222222, 'epoch': 17.32}
Step 122: Loss = 0.1118, LR = 2.22e-04
Step 122 (Epoch 18): Training Loss = 0.1180
{'loss': 0.118, 'grad_norm': 0.16755816340446472, 'learning_rate': 0.0002111111111111111, 'epoch': 17.48}
Step 123: Loss = 0.1180, LR = 2.11e-04
Step 123 (Epoch 18): Training Loss = 0.1170
{'loss': 0.117, 'grad_norm': 0.2719147801399231, 'learning_rate': 0.0002, 'epoch': 17.64}
Step 124: Loss = 0.1170, LR = 2.00e-04
Step 124 (Epoch 18): Training Loss = 0.1161
{'loss': 0.1161, 'grad_norm': 0.29311612248420715, 'learning_rate': 0.00018888888888888888, 'epoch': 17.8}
Step 125: Loss = 0.1161, LR = 1.89e-04
Step 125 (Epoch 18): Training Loss = 0.1139
{'loss': 0.1139, 'grad_norm': 0.16301488876342773, 'learning_rate': 0.00017777777777777779, 'epoch': 17.96}
Step 126: Loss = 0.1139, LR = 1.78e-04
Step 126 (Epoch 18): Training Loss = 0.1199
{'loss': 0.1199, 'grad_norm': 0.3174051344394684, 'learning_rate': 0.00016666666666666666, 'epoch': 18.0}

--- Starting Epoch 19 ---
Step 127: Loss = 0.1199, LR = 1.67e-04
Step 127 (Epoch 19): Training Loss = 0.1127
{'loss': 0.1127, 'grad_norm': 0.15058651566505432, 'learning_rate': 0.00015555555555555556, 'epoch': 18.16}
Step 128: Loss = 0.1127, LR = 1.56e-04
Step 128 (Epoch 19): Training Loss = 0.1129
{'loss': 0.1129, 'grad_norm': 0.15528440475463867, 'learning_rate': 0.00014444444444444444, 'epoch': 18.32}
Step 129: Loss = 0.1129, LR = 1.44e-04
Step 129 (Epoch 19): Training Loss = 0.1112
{'loss': 0.1112, 'grad_norm': 0.1347963660955429, 'learning_rate': 0.00013333333333333334, 'epoch': 18.48}
Step 130: Loss = 0.1112, LR = 1.33e-04
Step 130 (Epoch 19): Training Loss = 0.1128
{'loss': 0.1128, 'grad_norm': 0.1540174037218094, 'learning_rate': 0.00012222222222222221, 'epoch': 18.64}
Step 131: Loss = 0.1128, LR = 1.22e-04
Step 131 (Epoch 19): Training Loss = 0.1104
{'loss': 0.1104, 'grad_norm': 0.14462466537952423, 'learning_rate': 0.0001111111111111111, 'epoch': 18.8}
Step 132: Loss = 0.1104, LR = 1.11e-04
Step 132 (Epoch 19): Training Loss = 0.1170
{'loss': 0.117, 'grad_norm': 0.15728121995925903, 'learning_rate': 0.0001, 'epoch': 18.96}
Step 133: Loss = 0.1170, LR = 1.00e-04
Step 133 (Epoch 19): Training Loss = 0.1184
{'loss': 0.1184, 'grad_norm': 0.3315875828266144, 'learning_rate': 8.888888888888889e-05, 'epoch': 19.0}

--- Starting Epoch 20 ---
Step 134: Loss = 0.1184, LR = 8.89e-05
Step 134 (Epoch 20): Training Loss = 0.1117
{'loss': 0.1117, 'grad_norm': 0.16666221618652344, 'learning_rate': 7.777777777777778e-05, 'epoch': 19.16}
Step 135: Loss = 0.1117, LR = 7.78e-05
Step 135 (Epoch 20): Training Loss = 0.1152
{'loss': 0.1152, 'grad_norm': 0.148969367146492, 'learning_rate': 6.666666666666667e-05, 'epoch': 19.32}
Step 136: Loss = 0.1152, LR = 6.67e-05
Step 136 (Epoch 20): Training Loss = 0.1114
{'loss': 0.1114, 'grad_norm': 0.16028645634651184, 'learning_rate': 5.555555555555555e-05, 'epoch': 19.48}
Step 137: Loss = 0.1114, LR = 5.56e-05
Step 137 (Epoch 20): Training Loss = 0.1096
{'loss': 0.1096, 'grad_norm': 0.15241739153862, 'learning_rate': 4.4444444444444447e-05, 'epoch': 19.64}
Step 138: Loss = 0.1096, LR = 4.44e-05
Step 138 (Epoch 20): Training Loss = 0.1124
{'loss': 0.1124, 'grad_norm': 0.15613844990730286, 'learning_rate': 3.3333333333333335e-05, 'epoch': 19.8}
Step 139: Loss = 0.1124, LR = 3.33e-05
Step 139 (Epoch 20): Training Loss = 0.1119
{'loss': 0.1119, 'grad_norm': 0.15404364466667175, 'learning_rate': 2.2222222222222223e-05, 'epoch': 19.96}
Step 140: Loss = 0.1119, LR = 2.22e-05
Step 140 (Epoch 20): Training Loss = 0.1010
{'loss': 0.101, 'grad_norm': 0.3049660921096802, 'learning_rate': 1.1111111111111112e-05, 'epoch': 20.0}
{'train_runtime': 556.5854, 'train_samples_per_second': 3.593, 'train_steps_per_second': 0.252, 'train_loss': 0.6902954909418311, 'epoch': 20.0}
==================================================
Saving adapter...
Training complete! Adapter saved to /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: niah_single_2
max_samples: 100
output_file: benchmark_results.json
benchmark: all
perplexity_texts: None
mmlu_subset: all
glue_task: all
batch_size: 2
Using data file: needles/2048/niah_single_2_2048.json
Loading model...

=== Running QA Benchmark ===
Loading test data...
Loading QA data from needles/2048/niah_single_2_2048.json
Successfully loaded 100 items from JSON file
Evaluating on 100 samples
Generating predictions with batch size 2...

--- Sample Results ---

Sample 1:
Question: What is the special magic number for watery-walk mentioned in the provided text?
Expected: 2166941
Predicted: 5107245

Sample 2:
Question: What is the special magic number for lovely-terrorist mentioned in the provided text?
Expected: 5663623
Predicted: 5663623

Sample 3:
Question: What is the special magic number for discreet-barometer mentioned in the provided text?
Expected: 7210606
Predicted: 8973915
Evaluating...

=== QA Benchmark Results ===
Exact Accuracy: 0.180
Partial Accuracy: 0.180
Exact Matches: 18/100
Partial Matches: 0/100

=== Running Perplexity Benchmark ===
Calculating perplexity on 5 texts...
Perplexity: 443.303
Average Loss: 6.094
Total Tokens: 54

=== Running GLUE Benchmark ===
Running GLUE task: sst2
