Using data file: needles/2048/qa_1_2048.json
Auto-generated QA directory: outputs/qa_1_2048
Loading data...
Successfully loaded 100 items from JSON file
Loaded 100 generated QA snippets from outputs/qa_1_2048
Loaded 100 examples
Creating model...
Trainable parameters: 21,823,488
Total parameters: 4,739,675,136
Trainable %: 0.46%
Starting training...
Total training steps: 120
==================================================

--- Starting Epoch 1 ---
Step 1 (Epoch 1): Training Loss = 4.5783
{'loss': 4.5783, 'grad_norm': 7.9341912269592285, 'learning_rate': 0.0, 'epoch': 0.16}
Step 2: Loss = 4.5783, LR = 0.00e+00
Step 2 (Epoch 1): Training Loss = 5.4622
{'loss': 5.4622, 'grad_norm': 11.906692504882812, 'learning_rate': 2e-05, 'epoch': 0.32}
Step 3: Loss = 5.4622, LR = 2.00e-05
Step 3 (Epoch 1): Training Loss = 4.2925
{'loss': 4.2925, 'grad_norm': nan, 'learning_rate': 4e-05, 'epoch': 0.48}
Step 4: Loss = 4.2925, LR = 4.00e-05
Step 4 (Epoch 1): Training Loss = 4.9721
{'loss': 4.9721, 'grad_norm': 11.532242774963379, 'learning_rate': 6e-05, 'epoch': 0.64}
Step 5: Loss = 4.9721, LR = 6.00e-05
Step 5 (Epoch 1): Training Loss = 3.4056
{'loss': 3.4056, 'grad_norm': 4.715167999267578, 'learning_rate': 8e-05, 'epoch': 0.8}
Step 6: Loss = 3.4056, LR = 8.00e-05
Step 6 (Epoch 1): Training Loss = 3.7906
{'loss': 3.7906, 'grad_norm': 10.62658977508545, 'learning_rate': 0.0001, 'epoch': 0.96}
Step 7: Loss = 3.7906, LR = 1.00e-04
Step 7 (Epoch 1): Training Loss = 3.2606
{'loss': 3.2606, 'grad_norm': nan, 'learning_rate': 0.00012, 'epoch': 1.0}

--- Starting Epoch 2 ---
Step 8: Loss = 3.2606, LR = 1.20e-04
Step 8 (Epoch 2): Training Loss = 2.6085
{'loss': 2.6085, 'grad_norm': 4.1700544357299805, 'learning_rate': 0.00014000000000000001, 'epoch': 1.16}
Step 9: Loss = 2.6085, LR = 1.40e-04
Step 9 (Epoch 2): Training Loss = 2.2454
{'loss': 2.2454, 'grad_norm': 2.4508464336395264, 'learning_rate': 0.00016, 'epoch': 1.32}
Step 10: Loss = 2.2454, LR = 1.60e-04
Step 10 (Epoch 2): Training Loss = 2.1206
{'loss': 2.1206, 'grad_norm': 1.0525985956192017, 'learning_rate': 0.00017999999999999998, 'epoch': 1.48}
Step 11: Loss = 2.1206, LR = 1.80e-04
Step 11 (Epoch 2): Training Loss = 2.0408
{'loss': 2.0408, 'grad_norm': 0.8130186200141907, 'learning_rate': 0.0002, 'epoch': 1.64}
Step 12: Loss = 2.0408, LR = 2.00e-04
Step 12 (Epoch 2): Training Loss = 1.8037
{'loss': 1.8037, 'grad_norm': 0.7678300142288208, 'learning_rate': 0.00022, 'epoch': 1.8}
Step 13: Loss = 1.8037, LR = 2.20e-04
Step 13 (Epoch 2): Training Loss = 1.7721
{'loss': 1.7721, 'grad_norm': 0.7415408492088318, 'learning_rate': 0.00024, 'epoch': 1.96}
Step 14: Loss = 1.7721, LR = 2.40e-04
Step 14 (Epoch 2): Training Loss = 1.4964
{'loss': 1.4964, 'grad_norm': 0.61630779504776, 'learning_rate': 0.00026000000000000003, 'epoch': 2.0}

--- Starting Epoch 3 ---
Step 15: Loss = 1.4964, LR = 2.60e-04
Step 15 (Epoch 3): Training Loss = 1.6431
{'loss': 1.6431, 'grad_norm': 0.5631149411201477, 'learning_rate': 0.00028000000000000003, 'epoch': 2.16}
Step 16: Loss = 1.6431, LR = 2.80e-04
Step 16 (Epoch 3): Training Loss = 1.5340
{'loss': 1.534, 'grad_norm': 0.4728107452392578, 'learning_rate': 0.0003, 'epoch': 2.32}
Step 17: Loss = 1.5340, LR = 3.00e-04
Step 17 (Epoch 3): Training Loss = 1.3690
{'loss': 1.369, 'grad_norm': 0.47070199251174927, 'learning_rate': 0.00032, 'epoch': 2.48}
Step 18: Loss = 1.3690, LR = 3.20e-04
Step 18 (Epoch 3): Training Loss = 1.4626
{'loss': 1.4626, 'grad_norm': 0.5517648458480835, 'learning_rate': 0.00034, 'epoch': 2.64}
Step 19: Loss = 1.4626, LR = 3.40e-04
Step 19 (Epoch 3): Training Loss = 1.2077
{'loss': 1.2077, 'grad_norm': 0.5018102526664734, 'learning_rate': 0.00035999999999999997, 'epoch': 2.8}
Step 20: Loss = 1.2077, LR = 3.60e-04
Step 20 (Epoch 3): Training Loss = 1.2847
{'loss': 1.2847, 'grad_norm': 0.6045609712600708, 'learning_rate': 0.00038, 'epoch': 2.96}
Step 21: Loss = 1.2847, LR = 3.80e-04
Step 21 (Epoch 3): Training Loss = 0.8335
{'loss': 0.8335, 'grad_norm': 1.0687191486358643, 'learning_rate': 0.0004, 'epoch': 3.0}

--- Starting Epoch 4 ---
Step 22: Loss = 0.8335, LR = 4.00e-04
Step 22 (Epoch 4): Training Loss = 1.1735
{'loss': 1.1735, 'grad_norm': 0.42554959654808044, 'learning_rate': 0.00042, 'epoch': 3.16}
Step 23: Loss = 1.1735, LR = 4.20e-04
Step 23 (Epoch 4): Training Loss = 1.1631
{'loss': 1.1631, 'grad_norm': 0.3247911036014557, 'learning_rate': 0.00044, 'epoch': 3.32}
Step 24: Loss = 1.1631, LR = 4.40e-04
Step 24 (Epoch 4): Training Loss = 1.2359
{'loss': 1.2359, 'grad_norm': 0.27929818630218506, 'learning_rate': 0.00046, 'epoch': 3.48}
Step 25: Loss = 1.2359, LR = 4.60e-04
Step 25 (Epoch 4): Training Loss = 1.1819
{'loss': 1.1819, 'grad_norm': 0.30136099457740784, 'learning_rate': 0.00048, 'epoch': 3.64}
Step 26: Loss = 1.1819, LR = 4.80e-04
Step 26 (Epoch 4): Training Loss = 1.1309
{'loss': 1.1309, 'grad_norm': 0.3934720754623413, 'learning_rate': 0.0005, 'epoch': 3.8}
Step 27: Loss = 1.1309, LR = 5.00e-04
Step 27 (Epoch 4): Training Loss = 1.0033
{'loss': 1.0033, 'grad_norm': 0.32752206921577454, 'learning_rate': 0.0005200000000000001, 'epoch': 3.96}
Step 28: Loss = 1.0033, LR = 5.20e-04
Step 28 (Epoch 4): Training Loss = 1.0354
{'loss': 1.0354, 'grad_norm': 0.5054475665092468, 'learning_rate': 0.00054, 'epoch': 4.0}

--- Starting Epoch 5 ---
Step 29: Loss = 1.0354, LR = 5.40e-04
Step 29 (Epoch 5): Training Loss = 1.0286
{'loss': 1.0286, 'grad_norm': 0.31997960805892944, 'learning_rate': 0.0005600000000000001, 'epoch': 4.16}
Step 30: Loss = 1.0286, LR = 5.60e-04
Step 30 (Epoch 5): Training Loss = 0.9163
{'loss': 0.9163, 'grad_norm': 0.3967306911945343, 'learning_rate': 0.00058, 'epoch': 4.32}
Step 31: Loss = 0.9163, LR = 5.80e-04
Step 31 (Epoch 5): Training Loss = 0.8725
{'loss': 0.8725, 'grad_norm': 0.4013412296772003, 'learning_rate': 0.0006, 'epoch': 4.48}
Step 32: Loss = 0.8725, LR = 6.00e-04
Step 32 (Epoch 5): Training Loss = 0.9807
{'loss': 0.9807, 'grad_norm': 0.4061683714389801, 'learning_rate': 0.00062, 'epoch': 4.64}
Step 33: Loss = 0.9807, LR = 6.20e-04
Step 33 (Epoch 5): Training Loss = 0.9847
{'loss': 0.9847, 'grad_norm': 0.4591134786605835, 'learning_rate': 0.00064, 'epoch': 4.8}
Step 34: Loss = 0.9847, LR = 6.40e-04
Step 34 (Epoch 5): Training Loss = 0.9095
{'loss': 0.9095, 'grad_norm': 0.4231739342212677, 'learning_rate': 0.00066, 'epoch': 4.96}
Step 35: Loss = 0.9095, LR = 6.60e-04
Step 35 (Epoch 5): Training Loss = 0.8494
{'loss': 0.8494, 'grad_norm': 0.8219661712646484, 'learning_rate': 0.00068, 'epoch': 5.0}

--- Starting Epoch 6 ---
Step 36: Loss = 0.8494, LR = 6.80e-04
Step 36 (Epoch 6): Training Loss = 0.7837
{'loss': 0.7837, 'grad_norm': 0.43449410796165466, 'learning_rate': 0.0007, 'epoch': 5.16}
Step 37: Loss = 0.7837, LR = 7.00e-04
Step 37 (Epoch 6): Training Loss = 0.7440
{'loss': 0.744, 'grad_norm': 0.5404982566833496, 'learning_rate': 0.0007199999999999999, 'epoch': 5.32}
Step 38: Loss = 0.7440, LR = 7.20e-04
Step 38 (Epoch 6): Training Loss = 0.6846
{'loss': 0.6846, 'grad_norm': 0.4635724723339081, 'learning_rate': 0.00074, 'epoch': 5.48}
Step 39: Loss = 0.6846, LR = 7.40e-04
Step 39 (Epoch 6): Training Loss = 0.7904
{'loss': 0.7904, 'grad_norm': 0.6354358196258545, 'learning_rate': 0.00076, 'epoch': 5.64}
Step 40: Loss = 0.7904, LR = 7.60e-04
Step 40 (Epoch 6): Training Loss = 0.6608
{'loss': 0.6608, 'grad_norm': 0.663333535194397, 'learning_rate': 0.0007800000000000001, 'epoch': 5.8}
Step 41: Loss = 0.6608, LR = 7.80e-04
Step 41 (Epoch 6): Training Loss = 0.8000
{'loss': 0.8, 'grad_norm': 0.7531610727310181, 'learning_rate': 0.0008, 'epoch': 5.96}
Step 42: Loss = 0.8000, LR = 8.00e-04
Step 42 (Epoch 6): Training Loss = 0.7555
{'loss': 0.7555, 'grad_norm': 0.8997455835342407, 'learning_rate': 0.00082, 'epoch': 6.0}

--- Starting Epoch 7 ---
Step 43: Loss = 0.7555, LR = 8.20e-04
Step 43 (Epoch 7): Training Loss = 0.5861
{'loss': 0.5861, 'grad_norm': 0.4961349070072174, 'learning_rate': 0.00084, 'epoch': 6.16}
Step 44: Loss = 0.5861, LR = 8.40e-04
Step 44 (Epoch 7): Training Loss = 0.5585
{'loss': 0.5585, 'grad_norm': 0.4806770384311676, 'learning_rate': 0.00086, 'epoch': 6.32}
Step 45: Loss = 0.5585, LR = 8.60e-04
Step 45 (Epoch 7): Training Loss = 0.6092
{'loss': 0.6092, 'grad_norm': 0.6627520322799683, 'learning_rate': 0.00088, 'epoch': 6.48}
Step 46: Loss = 0.6092, LR = 8.80e-04
Step 46 (Epoch 7): Training Loss = 0.6027
{'loss': 0.6027, 'grad_norm': 0.6387448310852051, 'learning_rate': 0.0009000000000000001, 'epoch': 6.64}
Step 47: Loss = 0.6027, LR = 9.00e-04
Step 47 (Epoch 7): Training Loss = 0.5290
{'loss': 0.529, 'grad_norm': 0.6223072409629822, 'learning_rate': 0.00092, 'epoch': 6.8}
Step 48: Loss = 0.5290, LR = 9.20e-04
Step 48 (Epoch 7): Training Loss = 0.6108
{'loss': 0.6108, 'grad_norm': 0.6390669345855713, 'learning_rate': 0.00094, 'epoch': 6.96}
Step 49: Loss = 0.6108, LR = 9.40e-04
Step 49 (Epoch 7): Training Loss = 0.5987
{'loss': 0.5987, 'grad_norm': 0.9308794140815735, 'learning_rate': 0.00096, 'epoch': 7.0}

--- Starting Epoch 8 ---
Step 50: Loss = 0.5987, LR = 9.60e-04
Step 50 (Epoch 8): Training Loss = 0.4016
{'loss': 0.4016, 'grad_norm': 0.6364446878433228, 'learning_rate': 0.00098, 'epoch': 7.16}
Step 51: Loss = 0.4016, LR = 9.80e-04
Step 51 (Epoch 8): Training Loss = 0.3933
{'loss': 0.3933, 'grad_norm': 0.5848754644393921, 'learning_rate': 0.001, 'epoch': 7.32}
Step 52: Loss = 0.3933, LR = 1.00e-03
Step 52 (Epoch 8): Training Loss = 0.4344
{'loss': 0.4344, 'grad_norm': 0.6313619613647461, 'learning_rate': 0.000988888888888889, 'epoch': 7.48}
Step 53: Loss = 0.4344, LR = 9.89e-04
Step 53 (Epoch 8): Training Loss = 0.4440
{'loss': 0.444, 'grad_norm': 0.6512497663497925, 'learning_rate': 0.0009777777777777777, 'epoch': 7.64}
Step 54: Loss = 0.4440, LR = 9.78e-04
Step 54 (Epoch 8): Training Loss = 0.3941
{'loss': 0.3941, 'grad_norm': 0.5416552424430847, 'learning_rate': 0.0009666666666666667, 'epoch': 7.8}
Step 55: Loss = 0.3941, LR = 9.67e-04
Step 55 (Epoch 8): Training Loss = 0.4157
{'loss': 0.4157, 'grad_norm': 0.5497889518737793, 'learning_rate': 0.0009555555555555556, 'epoch': 7.96}
Step 56: Loss = 0.4157, LR = 9.56e-04
Step 56 (Epoch 8): Training Loss = 0.5360
{'loss': 0.536, 'grad_norm': 1.2155811786651611, 'learning_rate': 0.0009444444444444445, 'epoch': 8.0}

--- Starting Epoch 9 ---
Step 57: Loss = 0.5360, LR = 9.44e-04
Step 57 (Epoch 9): Training Loss = 0.3225
{'loss': 0.3225, 'grad_norm': 0.561057448387146, 'learning_rate': 0.0009333333333333333, 'epoch': 8.16}
Step 58: Loss = 0.3225, LR = 9.33e-04
Step 58 (Epoch 9): Training Loss = 0.2600
{'loss': 0.26, 'grad_norm': 0.4850468635559082, 'learning_rate': 0.0009222222222222223, 'epoch': 8.32}
Step 59: Loss = 0.2600, LR = 9.22e-04
Step 59 (Epoch 9): Training Loss = 0.2303
{'loss': 0.2303, 'grad_norm': 0.5051137804985046, 'learning_rate': 0.0009111111111111111, 'epoch': 8.48}
Step 60: Loss = 0.2303, LR = 9.11e-04
Step 60 (Epoch 9): Training Loss = 0.3195
{'loss': 0.3195, 'grad_norm': 0.7350528240203857, 'learning_rate': 0.0009000000000000001, 'epoch': 8.64}
Step 61: Loss = 0.3195, LR = 9.00e-04
Step 61 (Epoch 9): Training Loss = 0.2844
{'loss': 0.2844, 'grad_norm': 0.640017032623291, 'learning_rate': 0.0008888888888888888, 'epoch': 8.8}
Step 62: Loss = 0.2844, LR = 8.89e-04
Step 62 (Epoch 9): Training Loss = 0.3095
{'loss': 0.3095, 'grad_norm': 0.5839976668357849, 'learning_rate': 0.0008777777777777778, 'epoch': 8.96}
Step 63: Loss = 0.3095, LR = 8.78e-04
Step 63 (Epoch 9): Training Loss = 0.3109
{'loss': 0.3109, 'grad_norm': 0.9721137285232544, 'learning_rate': 0.0008666666666666667, 'epoch': 9.0}

--- Starting Epoch 10 ---
Step 64: Loss = 0.3109, LR = 8.67e-04
Step 64 (Epoch 10): Training Loss = 0.2081
{'loss': 0.2081, 'grad_norm': 0.4901280701160431, 'learning_rate': 0.0008555555555555556, 'epoch': 9.16}
Step 65: Loss = 0.2081, LR = 8.56e-04
Step 65 (Epoch 10): Training Loss = 0.2223
{'loss': 0.2223, 'grad_norm': 0.8197021484375, 'learning_rate': 0.0008444444444444444, 'epoch': 9.32}
Step 66: Loss = 0.2223, LR = 8.44e-04
Step 66 (Epoch 10): Training Loss = 0.1903
{'loss': 0.1903, 'grad_norm': 0.4846125543117523, 'learning_rate': 0.0008333333333333334, 'epoch': 9.48}
Step 67: Loss = 0.1903, LR = 8.33e-04
Step 67 (Epoch 10): Training Loss = 0.2029
{'loss': 0.2029, 'grad_norm': 0.5433230996131897, 'learning_rate': 0.0008222222222222222, 'epoch': 9.64}
Step 68: Loss = 0.2029, LR = 8.22e-04
Step 68 (Epoch 10): Training Loss = 0.1827
{'loss': 0.1827, 'grad_norm': 0.6494444012641907, 'learning_rate': 0.0008111111111111111, 'epoch': 9.8}
Step 69: Loss = 0.1827, LR = 8.11e-04
Step 69 (Epoch 10): Training Loss = 0.1740
{'loss': 0.174, 'grad_norm': 0.5912086367607117, 'learning_rate': 0.0008, 'epoch': 9.96}
Step 70: Loss = 0.1740, LR = 8.00e-04
Step 70 (Epoch 10): Training Loss = 0.2342
{'loss': 0.2342, 'grad_norm': 1.0726159811019897, 'learning_rate': 0.0007888888888888889, 'epoch': 10.0}

--- Starting Epoch 11 ---
Step 71: Loss = 0.2342, LR = 7.89e-04
Step 71 (Epoch 11): Training Loss = 0.1200
{'loss': 0.12, 'grad_norm': 0.40965500473976135, 'learning_rate': 0.0007777777777777778, 'epoch': 10.16}
Step 72: Loss = 0.1200, LR = 7.78e-04
Step 72 (Epoch 11): Training Loss = 0.1523
{'loss': 0.1523, 'grad_norm': 0.5122029781341553, 'learning_rate': 0.0007666666666666667, 'epoch': 10.32}
Step 73: Loss = 0.1523, LR = 7.67e-04
Step 73 (Epoch 11): Training Loss = 0.1296
{'loss': 0.1296, 'grad_norm': 0.4626791775226593, 'learning_rate': 0.0007555555555555555, 'epoch': 10.48}
Step 74: Loss = 0.1296, LR = 7.56e-04
Step 74 (Epoch 11): Training Loss = 0.1196
{'loss': 0.1196, 'grad_norm': 0.4916364252567291, 'learning_rate': 0.0007444444444444445, 'epoch': 10.64}
Step 75: Loss = 0.1196, LR = 7.44e-04
Step 75 (Epoch 11): Training Loss = 0.1089
{'loss': 0.1089, 'grad_norm': 0.4313820004463196, 'learning_rate': 0.0007333333333333333, 'epoch': 10.8}
Step 76: Loss = 0.1089, LR = 7.33e-04
Step 76 (Epoch 11): Training Loss = 0.1177
{'loss': 0.1177, 'grad_norm': 0.5133280158042908, 'learning_rate': 0.0007222222222222222, 'epoch': 10.96}
Step 77: Loss = 0.1177, LR = 7.22e-04
Step 77 (Epoch 11): Training Loss = 0.1060
{'loss': 0.106, 'grad_norm': 0.8674731254577637, 'learning_rate': 0.0007111111111111111, 'epoch': 11.0}

--- Starting Epoch 12 ---
Step 78: Loss = 0.1060, LR = 7.11e-04
Step 78 (Epoch 12): Training Loss = 0.0567
{'loss': 0.0567, 'grad_norm': 0.28285858035087585, 'learning_rate': 0.0007, 'epoch': 11.16}
Step 79: Loss = 0.0567, LR = 7.00e-04
Step 79 (Epoch 12): Training Loss = 0.0829
{'loss': 0.0829, 'grad_norm': 0.49104294180870056, 'learning_rate': 0.000688888888888889, 'epoch': 11.32}
Step 80: Loss = 0.0829, LR = 6.89e-04
Step 80 (Epoch 12): Training Loss = 0.0839
{'loss': 0.0839, 'grad_norm': 0.4074214696884155, 'learning_rate': 0.0006777777777777778, 'epoch': 11.48}
Step 81: Loss = 0.0839, LR = 6.78e-04
Step 81 (Epoch 12): Training Loss = 0.0741
{'loss': 0.0741, 'grad_norm': 0.4312690496444702, 'learning_rate': 0.0006666666666666666, 'epoch': 11.64}
Step 82: Loss = 0.0741, LR = 6.67e-04
Step 82 (Epoch 12): Training Loss = 0.0668
{'loss': 0.0668, 'grad_norm': 0.44010940194129944, 'learning_rate': 0.0006555555555555556, 'epoch': 11.8}
Step 83: Loss = 0.0668, LR = 6.56e-04
Step 83 (Epoch 12): Training Loss = 0.0690
{'loss': 0.069, 'grad_norm': 0.41461533308029175, 'learning_rate': 0.0006444444444444444, 'epoch': 11.96}
Step 84: Loss = 0.0690, LR = 6.44e-04
Step 84 (Epoch 12): Training Loss = 0.0673
{'loss': 0.0673, 'grad_norm': 0.7175167798995972, 'learning_rate': 0.0006333333333333333, 'epoch': 12.0}

--- Starting Epoch 13 ---
Step 85: Loss = 0.0673, LR = 6.33e-04
Step 85 (Epoch 13): Training Loss = 0.0373
{'loss': 0.0373, 'grad_norm': 0.25902941823005676, 'learning_rate': 0.0006222222222222223, 'epoch': 12.16}
Step 86: Loss = 0.0373, LR = 6.22e-04
Step 86 (Epoch 13): Training Loss = 0.0425
{'loss': 0.0425, 'grad_norm': 0.3052501082420349, 'learning_rate': 0.0006111111111111112, 'epoch': 12.32}
Step 87: Loss = 0.0425, LR = 6.11e-04
Step 87 (Epoch 13): Training Loss = 0.0506
{'loss': 0.0506, 'grad_norm': 0.3509693443775177, 'learning_rate': 0.0006, 'epoch': 12.48}
Step 88: Loss = 0.0506, LR = 6.00e-04
Step 88 (Epoch 13): Training Loss = 0.0460
{'loss': 0.046, 'grad_norm': 0.3633962869644165, 'learning_rate': 0.0005888888888888889, 'epoch': 12.64}
Step 89: Loss = 0.0460, LR = 5.89e-04
Step 89 (Epoch 13): Training Loss = 0.0545
{'loss': 0.0545, 'grad_norm': 0.5904654264450073, 'learning_rate': 0.0005777777777777778, 'epoch': 12.8}
Step 90: Loss = 0.0545, LR = 5.78e-04
Step 90 (Epoch 13): Training Loss = 0.0492
{'loss': 0.0492, 'grad_norm': 0.3539775609970093, 'learning_rate': 0.0005666666666666667, 'epoch': 12.96}
Step 91: Loss = 0.0492, LR = 5.67e-04
Step 91 (Epoch 13): Training Loss = 0.0381
{'loss': 0.0381, 'grad_norm': 0.5344640612602234, 'learning_rate': 0.0005555555555555556, 'epoch': 13.0}

--- Starting Epoch 14 ---
Step 92: Loss = 0.0381, LR = 5.56e-04
Step 92 (Epoch 14): Training Loss = 0.0344
{'loss': 0.0344, 'grad_norm': 0.2794017791748047, 'learning_rate': 0.0005444444444444444, 'epoch': 13.16}
Step 93: Loss = 0.0344, LR = 5.44e-04
Step 93 (Epoch 14): Training Loss = 0.0368
{'loss': 0.0368, 'grad_norm': 0.3201390206813812, 'learning_rate': 0.0005333333333333334, 'epoch': 13.32}
Step 94: Loss = 0.0368, LR = 5.33e-04
Step 94 (Epoch 14): Training Loss = 0.0326
{'loss': 0.0326, 'grad_norm': 0.2795126140117645, 'learning_rate': 0.0005222222222222223, 'epoch': 13.48}
Step 95: Loss = 0.0326, LR = 5.22e-04
Step 95 (Epoch 14): Training Loss = 0.0356
{'loss': 0.0356, 'grad_norm': 0.31274333596229553, 'learning_rate': 0.0005111111111111111, 'epoch': 13.64}
Step 96: Loss = 0.0356, LR = 5.11e-04
Step 96 (Epoch 14): Training Loss = 0.0354
{'loss': 0.0354, 'grad_norm': 0.31552594900131226, 'learning_rate': 0.0005, 'epoch': 13.8}
Step 97: Loss = 0.0354, LR = 5.00e-04
Step 97 (Epoch 14): Training Loss = 0.0360
{'loss': 0.036, 'grad_norm': 0.28052857518196106, 'learning_rate': 0.0004888888888888889, 'epoch': 13.96}
Step 98: Loss = 0.0360, LR = 4.89e-04
Step 98 (Epoch 14): Training Loss = 0.0486
{'loss': 0.0486, 'grad_norm': 0.7634788155555725, 'learning_rate': 0.0004777777777777778, 'epoch': 14.0}

--- Starting Epoch 15 ---
Step 99: Loss = 0.0486, LR = 4.78e-04
Step 99 (Epoch 15): Training Loss = 0.0231
{'loss': 0.0231, 'grad_norm': 0.243011474609375, 'learning_rate': 0.00046666666666666666, 'epoch': 14.16}
Step 100: Loss = 0.0231, LR = 4.67e-04
Step 100 (Epoch 15): Training Loss = 0.0248
{'loss': 0.0248, 'grad_norm': 0.1976684331893921, 'learning_rate': 0.00045555555555555556, 'epoch': 14.32}
Step 101: Loss = 0.0248, LR = 4.56e-04
Step 101 (Epoch 15): Training Loss = 0.0240
{'loss': 0.024, 'grad_norm': 0.25523099303245544, 'learning_rate': 0.0004444444444444444, 'epoch': 14.48}
Step 102: Loss = 0.0240, LR = 4.44e-04
Step 102 (Epoch 15): Training Loss = 0.0280
{'loss': 0.028, 'grad_norm': 0.25483641028404236, 'learning_rate': 0.00043333333333333337, 'epoch': 14.64}
Step 103: Loss = 0.0280, LR = 4.33e-04
Step 103 (Epoch 15): Training Loss = 0.0297
{'loss': 0.0297, 'grad_norm': 0.22125834226608276, 'learning_rate': 0.0004222222222222222, 'epoch': 14.8}
Step 104: Loss = 0.0297, LR = 4.22e-04
Step 104 (Epoch 15): Training Loss = 0.0263
{'loss': 0.0263, 'grad_norm': 0.2113710194826126, 'learning_rate': 0.0004111111111111111, 'epoch': 14.96}
Step 105: Loss = 0.0263, LR = 4.11e-04
Step 105 (Epoch 15): Training Loss = 0.0427
{'loss': 0.0427, 'grad_norm': 0.5130198001861572, 'learning_rate': 0.0004, 'epoch': 15.0}

--- Starting Epoch 16 ---
Step 106: Loss = 0.0427, LR = 4.00e-04
Step 106 (Epoch 16): Training Loss = 0.0187
{'loss': 0.0187, 'grad_norm': 0.11135967075824738, 'learning_rate': 0.0003888888888888889, 'epoch': 15.16}
Step 107: Loss = 0.0187, LR = 3.89e-04
Step 107 (Epoch 16): Training Loss = 0.0208
{'loss': 0.0208, 'grad_norm': 0.1742711216211319, 'learning_rate': 0.00037777777777777777, 'epoch': 15.32}
Step 108: Loss = 0.0208, LR = 3.78e-04
Step 108 (Epoch 16): Training Loss = 0.0197
{'loss': 0.0197, 'grad_norm': 0.1620018035173416, 'learning_rate': 0.00036666666666666667, 'epoch': 15.48}
Step 109: Loss = 0.0197, LR = 3.67e-04
Step 109 (Epoch 16): Training Loss = 0.0216
{'loss': 0.0216, 'grad_norm': 0.18974681198596954, 'learning_rate': 0.00035555555555555557, 'epoch': 15.64}
Step 110: Loss = 0.0216, LR = 3.56e-04
Step 110 (Epoch 16): Training Loss = 0.0244
{'loss': 0.0244, 'grad_norm': 0.23200172185897827, 'learning_rate': 0.0003444444444444445, 'epoch': 15.8}
Step 111: Loss = 0.0244, LR = 3.44e-04
Step 111 (Epoch 16): Training Loss = 0.0201
{'loss': 0.0201, 'grad_norm': 0.16669173538684845, 'learning_rate': 0.0003333333333333333, 'epoch': 15.96}
Step 112: Loss = 0.0201, LR = 3.33e-04
Step 112 (Epoch 16): Training Loss = 0.0183
{'loss': 0.0183, 'grad_norm': 0.5410959720611572, 'learning_rate': 0.0003222222222222222, 'epoch': 16.0}

--- Starting Epoch 17 ---
Step 113: Loss = 0.0183, LR = 3.22e-04
Step 113 (Epoch 17): Training Loss = 0.0156
{'loss': 0.0156, 'grad_norm': 0.08658911287784576, 'learning_rate': 0.0003111111111111111, 'epoch': 16.16}
Step 114: Loss = 0.0156, LR = 3.11e-04
Step 114 (Epoch 17): Training Loss = 0.0163
{'loss': 0.0163, 'grad_norm': 0.09622806310653687, 'learning_rate': 0.0003, 'epoch': 16.32}
Step 115: Loss = 0.0163, LR = 3.00e-04
Step 115 (Epoch 17): Training Loss = 0.0178
{'loss': 0.0178, 'grad_norm': 0.10278437286615372, 'learning_rate': 0.0002888888888888889, 'epoch': 16.48}
Step 116: Loss = 0.0178, LR = 2.89e-04
Step 116 (Epoch 17): Training Loss = 0.0195
{'loss': 0.0195, 'grad_norm': 0.15691205859184265, 'learning_rate': 0.0002777777777777778, 'epoch': 16.64}
Step 117: Loss = 0.0195, LR = 2.78e-04
Step 117 (Epoch 17): Training Loss = 0.0164
{'loss': 0.0164, 'grad_norm': 0.11188424378633499, 'learning_rate': 0.0002666666666666667, 'epoch': 16.8}
Step 118: Loss = 0.0164, LR = 2.67e-04
Step 118 (Epoch 17): Training Loss = 0.0158
{'loss': 0.0158, 'grad_norm': 0.09611482173204422, 'learning_rate': 0.00025555555555555553, 'epoch': 16.96}
Step 119: Loss = 0.0158, LR = 2.56e-04
Step 119 (Epoch 17): Training Loss = 0.0163
{'loss': 0.0163, 'grad_norm': 0.1789626181125641, 'learning_rate': 0.00024444444444444443, 'epoch': 17.0}

--- Starting Epoch 18 ---
Step 120: Loss = 0.0163, LR = 2.44e-04
Step 120 (Epoch 18): Training Loss = 0.0149
{'loss': 0.0149, 'grad_norm': 0.06743855774402618, 'learning_rate': 0.00023333333333333333, 'epoch': 17.16}
Step 121: Loss = 0.0149, LR = 2.33e-04
Step 121 (Epoch 18): Training Loss = 0.0152
{'loss': 0.0152, 'grad_norm': 0.0733499601483345, 'learning_rate': 0.0002222222222222222, 'epoch': 17.32}
Step 122: Loss = 0.0152, LR = 2.22e-04
Step 122 (Epoch 18): Training Loss = 0.0148
{'loss': 0.0148, 'grad_norm': 0.06031917780637741, 'learning_rate': 0.0002111111111111111, 'epoch': 17.48}
Step 123: Loss = 0.0148, LR = 2.11e-04
Step 123 (Epoch 18): Training Loss = 0.0130
{'loss': 0.013, 'grad_norm': 0.054704710841178894, 'learning_rate': 0.0002, 'epoch': 17.64}
Step 124: Loss = 0.0130, LR = 2.00e-04
Step 124 (Epoch 18): Training Loss = 0.0147
{'loss': 0.0147, 'grad_norm': 0.06649533659219742, 'learning_rate': 0.00018888888888888888, 'epoch': 17.8}
Step 125: Loss = 0.0147, LR = 1.89e-04
Step 125 (Epoch 18): Training Loss = 0.0131
{'loss': 0.0131, 'grad_norm': 0.05517797917127609, 'learning_rate': 0.00017777777777777779, 'epoch': 17.96}
Step 126: Loss = 0.0131, LR = 1.78e-04
Step 126 (Epoch 18): Training Loss = 0.0263
{'loss': 0.0263, 'grad_norm': 0.2679440677165985, 'learning_rate': 0.00016666666666666666, 'epoch': 18.0}

--- Starting Epoch 19 ---
Step 127: Loss = 0.0263, LR = 1.67e-04
Step 127 (Epoch 19): Training Loss = 0.0138
{'loss': 0.0138, 'grad_norm': 0.05408530309796333, 'learning_rate': 0.00015555555555555556, 'epoch': 18.16}
Step 128: Loss = 0.0138, LR = 1.56e-04
Step 128 (Epoch 19): Training Loss = 0.0141
{'loss': 0.0141, 'grad_norm': 0.05721122771501541, 'learning_rate': 0.00014444444444444444, 'epoch': 18.32}
Step 129: Loss = 0.0141, LR = 1.44e-04
Step 129 (Epoch 19): Training Loss = 0.0140
{'loss': 0.014, 'grad_norm': 0.08503460884094238, 'learning_rate': 0.00013333333333333334, 'epoch': 18.48}
Step 130: Loss = 0.0140, LR = 1.33e-04
Step 130 (Epoch 19): Training Loss = 0.0141
{'loss': 0.0141, 'grad_norm': 0.061371512711048126, 'learning_rate': 0.00012222222222222221, 'epoch': 18.64}
Step 131: Loss = 0.0141, LR = 1.22e-04
Step 131 (Epoch 19): Training Loss = 0.0138
{'loss': 0.0138, 'grad_norm': 0.05994490161538124, 'learning_rate': 0.0001111111111111111, 'epoch': 18.8}
Step 132: Loss = 0.0138, LR = 1.11e-04
Step 132 (Epoch 19): Training Loss = 0.0138
{'loss': 0.0138, 'grad_norm': 0.0668988972902298, 'learning_rate': 0.0001, 'epoch': 18.96}
Step 133: Loss = 0.0138, LR = 1.00e-04
Step 133 (Epoch 19): Training Loss = 0.0125
{'loss': 0.0125, 'grad_norm': 0.10246602445840836, 'learning_rate': 8.888888888888889e-05, 'epoch': 19.0}

--- Starting Epoch 20 ---
Step 134: Loss = 0.0125, LR = 8.89e-05
Step 134 (Epoch 20): Training Loss = 0.0135
{'loss': 0.0135, 'grad_norm': 0.05213921144604683, 'learning_rate': 7.777777777777778e-05, 'epoch': 19.16}
Step 135: Loss = 0.0135, LR = 7.78e-05
Step 135 (Epoch 20): Training Loss = 0.0135
{'loss': 0.0135, 'grad_norm': 0.05746861919760704, 'learning_rate': 6.666666666666667e-05, 'epoch': 19.32}
Step 136: Loss = 0.0135, LR = 6.67e-05
Step 136 (Epoch 20): Training Loss = 0.0130
{'loss': 0.013, 'grad_norm': 0.05359965190291405, 'learning_rate': 5.555555555555555e-05, 'epoch': 19.48}
Step 137: Loss = 0.0130, LR = 5.56e-05
Step 137 (Epoch 20): Training Loss = 0.0141
{'loss': 0.0141, 'grad_norm': 0.0609147809445858, 'learning_rate': 4.4444444444444447e-05, 'epoch': 19.64}
Step 138: Loss = 0.0141, LR = 4.44e-05
Step 138 (Epoch 20): Training Loss = 0.0131
{'loss': 0.0131, 'grad_norm': 0.046203818172216415, 'learning_rate': 3.3333333333333335e-05, 'epoch': 19.8}
Step 139: Loss = 0.0131, LR = 3.33e-05
Step 139 (Epoch 20): Training Loss = 0.0139
{'loss': 0.0139, 'grad_norm': 0.05698927119374275, 'learning_rate': 2.2222222222222223e-05, 'epoch': 19.96}
Step 140: Loss = 0.0139, LR = 2.22e-05
Step 140 (Epoch 20): Training Loss = 0.0130
{'loss': 0.013, 'grad_norm': 0.10721245408058167, 'learning_rate': 1.1111111111111112e-05, 'epoch': 20.0}
{'train_runtime': 1444.4978, 'train_samples_per_second': 1.385, 'train_steps_per_second': 0.097, 'train_loss': 0.6152054797591907, 'epoch': 20.0}
==================================================
Saving adapter...
Training complete! Adapter saved to /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA
Starting LoRA training with settings...
Needle size: 2048, Needle type: qa_1
Epochs: 20, Batch size: 2, Learning rate: 0.001
Command: python train_lora_optimized.py --model_name Qwen/Qwen3-8B --needle_size 2048 --needle_type qa_1 --epochs 20 --batch_size 2 --learning_rate 0.001 --adapter_r 8 --adapter_layers 10 --output_dir /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA --use_generated_qa

Training completed successfully!
