Using data file: needles/32768/niah_single_1_32768.json
Loading data...
Starting LoRA training with RTX 3090 optimized settings...
Needle size: 32768, Needle type: niah_single_1
Epochs: 5, Batch size: 2, Learning rate: 0.001
Command: python train_lora_optimized.py --model_name Qwen/Qwen3-8B --needle_size 32768 --needle_type niah_single_1 --epochs 5 --batch_size 2 --learning_rate 0.001 --adapter_r 8 --adapter_layers 10 --output_dir adapter
Training failed with error: Command '['python', 'train_lora_optimized.py', '--model_name', 'Qwen/Qwen3-8B', '--needle_size', '32768', '--needle_type', 'niah_single_1', '--epochs', '5', '--batch_size', '2', '--learning_rate', '0.001', '--adapter_r', '8', '--adapter_layers', '10', '--output_dir', 'adapter']' returned non-zero exit status 1.
Starting LoRA training with RTX 3090 optimized settings...
Needle size: 32768, Needle type: niah_single_1
Epochs: 5, Batch size: 2, Learning rate: 0.001
Command: python train_lora_optimized.py --model_name Qwen/Qwen3-8B --needle_size 32768 --needle_type niah_single_1 --epochs 5 --batch_size 2 --learning_rate 0.001 --adapter_r 8 --adapter_layers 10 --output_dir adapter
Using data file: needles/32768/niah_single_1_32768.json
Loading data...
Successfully loaded 30 items from JSON file
Loaded 30 examples
Creating model...
Trainable parameters: 21,823,488
Total parameters: 4,739,675,136
Trainable %: 0.46%
Starting training...
Total training steps: 5
==================================================

--- Starting Epoch 1 ---
Step 1 (Epoch 1): Training Loss = 5.3772
{'loss': 5.3772, 'grad_norm': 3.2419211864471436, 'learning_rate': 0.0, 'epoch': 0.53}
Step 2: Loss = 5.3772, LR = 0.00e+00
Step 2 (Epoch 1): Training Loss = 5.4728
{'loss': 5.4728, 'grad_norm': 3.4926345348358154, 'learning_rate': 2e-05, 'epoch': 1.0}

--- Starting Epoch 2 ---
Step 3: Loss = 5.4728, LR = 2.00e-05
Step 3 (Epoch 2): Training Loss = 5.3122
{'loss': 5.3122, 'grad_norm': 3.172227621078491, 'learning_rate': 4e-05, 'epoch': 1.53}
Step 4: Loss = 5.3122, LR = 4.00e-05
Step 4 (Epoch 2): Training Loss = 5.2349
{'loss': 5.2349, 'grad_norm': 3.5490775108337402, 'learning_rate': 6e-05, 'epoch': 2.0}

--- Starting Epoch 3 ---
Step 5: Loss = 5.2349, LR = 6.00e-05
Step 5 (Epoch 3): Training Loss = 4.9491
{'loss': 4.9491, 'grad_norm': 3.43278169631958, 'learning_rate': 8e-05, 'epoch': 2.53}
Step 6: Loss = 4.9491, LR = 8.00e-05
Step 6 (Epoch 3): Training Loss = 4.5826
{'loss': 4.5826, 'grad_norm': 3.4921281337738037, 'learning_rate': 0.0001, 'epoch': 3.0}

--- Starting Epoch 4 ---
Step 7: Loss = 4.5826, LR = 1.00e-04
Step 7 (Epoch 4): Training Loss = 4.1246
{'loss': 4.1246, 'grad_norm': 3.4252078533172607, 'learning_rate': 0.00012, 'epoch': 3.53}
Step 8: Loss = 4.1246, LR = 1.20e-04
Step 8 (Epoch 4): Training Loss = 3.7573
{'loss': 3.7573, 'grad_norm': 3.112361431121826, 'learning_rate': 0.00014000000000000001, 'epoch': 4.0}

--- Starting Epoch 5 ---
Step 9: Loss = 3.7573, LR = 1.40e-04
Step 9 (Epoch 5): Training Loss = 3.3909
{'loss': 3.3909, 'grad_norm': 3.1408495903015137, 'learning_rate': 0.00016, 'epoch': 4.53}
Step 10: Loss = 3.3909, LR = 1.60e-04
Step 10 (Epoch 5): Training Loss = 3.0727
{'loss': 3.0727, 'grad_norm': 2.9742066860198975, 'learning_rate': 0.00017999999999999998, 'epoch': 5.0}
{'train_runtime': 48.1239, 'train_samples_per_second': 3.117, 'train_steps_per_second': 0.208, 'train_loss': 4.527443265914917, 'epoch': 5.0}
==================================================
Saving adapter...
Training complete! Adapter saved to adapter
Using data file: needles/2048/qa_1_2048.json
Loading model...

Detailed results saved to benchmark_results.json
Starting LoRA training with RTX 3090 optimized settings...
Needle size: 32768, Needle type: niah_single_1
Epochs: 5, Batch size: 2, Learning rate: 0.001
Command: python train_lora_optimized.py --model_name Qwen/Qwen3-8B --needle_size 32768 --needle_type niah_single_1 --epochs 5 --batch_size 2 --learning_rate 0.001 --adapter_r 8 --adapter_layers 10 --output_dir adapter

Training completed successfully!
Deleted adapter directory: adapter
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: adapters/ep5_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: all_benchmarks_results.json
benchmark: all
perplexity_texts: None
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

=== Running QA Benchmark ===
Loading test data...
Loading QA data from needles/2048/qa_1_2048.json
Successfully loaded 100 items from JSON file
Evaluating on 100 samples
Generating predictions with batch size 8...

--- Sample Results ---

Sample 1:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: In what country is Normandy located?
Expected: France
Predicted: France

Sample 2:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: When were the Normans in Normandy?
Expected: 10th and 11th centuries
Predicted: 10th and 11th centuries

Sample 3:
Question: Answer the question based on the given documents. Only give me the answer and do not output any other words.

Question: From which countries did the Norse originate?
Expected: Denmark, Iceland and Norway
Predicted: Denmark, Iceland, and Norway
Evaluating...

=== QA Benchmark Results ===
Exact Accuracy: 0.310
Partial Accuracy: 0.430
Exact Matches: 31/100
Partial Matches: 12/100

=== Running Perplexity Benchmark ===
Calculating perplexity on 5 texts...
Perplexity: 76.245
Average Loss: 4.334
Total Tokens: 54

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.285
sst2: 0.550
mrpc: 0.306
qnli: 0.000

Detailed results saved to all_benchmarks_results.json
