# Adapter Benchmarks Stream - 2025-09-27T13:13:43.845189
{"event": "adapter_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.20it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.27it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.30it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.42it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.83it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.56it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.

Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]
Generating test split:  23%|██▎       | 1000/4358 [00:00<00:00, 8133.35 examples/s]
Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 34145.77 examples/s]

Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]
Generating train split:  90%|████████▉ | 33000/36718 [00:00<00:00, 319167.36 examples/s]
Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 336212.39 examples/s]

Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]
Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 364848.65 examples/s]
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:21,  2.03s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.52it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.04it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.51it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  6.94it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.22it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.22it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08, 10.05it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.76it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.28it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.64it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.93it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 12.06it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.15it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.19it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.35it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.19it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:04<00:05, 11.59it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.37it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.70it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.34it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.39it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.60it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.63it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.33it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.21it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.83it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.60it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:06<00:03, 11.05it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.53it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.60it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.73it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.44it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:07<00:02, 11.73it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.96it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 11.02it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.02it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:08<00:01, 10.61it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.02it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.39it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.54it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.64it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.67it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.29it/s]
Perplexity: 14.273
Average Loss: 2.658
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.483
sst2: 0.700
mrpc: 0.748
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep0_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 14.273432405025368, "average_loss": 2.6583999355033434, "total_tokens": 11644}}
{"event": "result", "adapter": "ep0_bs2_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.4826558265582655, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7479674796747967, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.09s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.41it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.81it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.99it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.86it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.62it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.26it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.83it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.26it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.58it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.75it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 12.00it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.02it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.17it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.08it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.40it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.24it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.46it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.28it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.11it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.56it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.35it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.74it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.32it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.58it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.98it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.36it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.63it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.89it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.95it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.59it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.04it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.65it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.00it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.48it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.11it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.41it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.60it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.71it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.73it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.15it/s]
Perplexity: 14.273
Average Loss: 2.658
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep0_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.483
sst2: 0.700
mrpc: 0.748
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep0_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep0_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.273432405025368, "average_loss": 2.6583999355033434, "total_tokens": 11644}}
{"event": "result", "adapter": "ep0_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.4826558265582655, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7479674796747967, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:24,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.48it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.37it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.73it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.93it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.89it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.67it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.24it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.77it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.19it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.31it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.52it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.83it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.92it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.11it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.04it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.41it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.55it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.28it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.53it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.27it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.13it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.74it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.51it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.92it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.34it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.43it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.64it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.03it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.41it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.68it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.90it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.96it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.17it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01,  9.97it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01,  9.47it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01,  9.99it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.42it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.91it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.07it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.44it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.54it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.60it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.07it/s]
Perplexity: 20.043
Average Loss: 2.998
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.317
sst2: 0.700
mrpc: 0.250
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep10_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 20.0425938574836, "average_loss": 2.9978597018469735, "total_tokens": 11644}}
{"event": "result", "adapter": "ep10_bs2_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.31666666666666665, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.25, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-70", "path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10/checkpoint-70"}
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.10s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.46it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.39it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.78it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  8.00it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.99it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.74it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.39it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.91it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.31it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.58it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.93it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.99it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.14it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.06it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.38it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.53it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.21it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.27it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.52it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.27it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.12it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.71it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.48it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.91it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.32it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.41it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.63it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.03it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.39it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.64it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.87it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.94it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.56it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.02it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.60it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.27it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01,  9.64it/s]
Calculating perplexity:  84%|████████▍ | 84/100 [00:09<00:01,  9.56it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01,  9.54it/s]
Calculating perplexity:  86%|████████▌ | 86/100 [00:09<00:01,  9.62it/s]
Calculating perplexity:  88%|████████▊ | 88/100 [00:10<00:01, 10.53it/s]
Calculating perplexity:  90%|█████████ | 90/100 [00:10<00:00, 10.94it/s]
Calculating perplexity:  92%|█████████▏| 92/100 [00:10<00:00, 11.11it/s]
Calculating perplexity:  94%|█████████▍| 94/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  96%|█████████▌| 96/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  98%|█████████▊| 98/100 [00:10<00:00, 11.63it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00, 11.75it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.06it/s]
Perplexity: 20.043
Average Loss: 2.998
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.317
sst2: 0.700
mrpc: 0.250
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_glue.json
{"event": "result", "adapter": "checkpoint-70", "benchmark": "perplexity", "result": {"perplexity": 20.0425938574836, "average_loss": 2.9978597018469735, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-70", "benchmark": "glue", "result": {"average_score": 0.31666666666666665, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.25, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.04s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.51it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   7%|▋         | 7/100 [00:02<00:20,  4.60it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.30it/s]
Calculating perplexity:   9%|▉         | 9/100 [00:02<00:15,  6.02it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.66it/s]
Calculating perplexity:  11%|█         | 11/100 [00:03<00:12,  7.27it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.73it/s]
Calculating perplexity:  13%|█▎        | 13/100 [00:03<00:10,  8.20it/s]
Calculating perplexity:  15%|█▌        | 15/100 [00:03<00:08,  9.63it/s]
Calculating perplexity:  17%|█▋        | 17/100 [00:03<00:07, 10.45it/s]
Calculating perplexity:  19%|█▉        | 19/100 [00:03<00:07, 11.03it/s]
Calculating perplexity:  21%|██        | 21/100 [00:03<00:06, 11.37it/s]
Calculating perplexity:  23%|██▎       | 23/100 [00:04<00:06, 11.56it/s]
Calculating perplexity:  25%|██▌       | 25/100 [00:04<00:06, 11.64it/s]
Calculating perplexity:  27%|██▋       | 27/100 [00:04<00:06, 11.83it/s]
Calculating perplexity:  29%|██▉       | 29/100 [00:04<00:06, 11.81it/s]
Calculating perplexity:  31%|███       | 31/100 [00:04<00:05, 11.83it/s]
Calculating perplexity:  33%|███▎      | 33/100 [00:04<00:05, 11.89it/s]
Calculating perplexity:  35%|███▌      | 35/100 [00:05<00:05, 10.94it/s]
Calculating perplexity:  37%|███▋      | 37/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  39%|███▉      | 39/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  41%|████      | 41/100 [00:05<00:05, 11.08it/s]
Calculating perplexity:  43%|████▎     | 43/100 [00:05<00:05, 10.83it/s]
Calculating perplexity:  45%|████▌     | 45/100 [00:06<00:05, 10.49it/s]
Calculating perplexity:  47%|████▋     | 47/100 [00:06<00:05, 10.02it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.30it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.64it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.17it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.42it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.83it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.21it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.48it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.73it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.31it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.68it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.27it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.65it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.14it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.76it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.05it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.14it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.30it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.48it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.55it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.93it/s]
Perplexity: 42.335
Average Loss: 3.746
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep10_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.375
sst2: 0.560
mrpc: 0.564
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep10_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep10_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 42.335332018635796, "average_loss": 3.745622009714622, "total_tokens": 11644}}
{"event": "result", "adapter": "ep10_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.3745454545454545, "task_results": {"sst2": {"score": 0.56, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5636363636363636, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-70", "path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA/checkpoint-70"}
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:19,  2.01s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:29,  1.09it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.53it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.00it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.42it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.75it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.95it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.90it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.65it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.31it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.73it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.03it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.33it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.42it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.61it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.60it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.73it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.82it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.00it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.21it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.00it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.43it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.50it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.22it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.08it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.64it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.36it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.71it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.07it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.15it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.81it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.23it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.46it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.46it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.81it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.48it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.00it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.24it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01,  9.93it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01,  9.96it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.12it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.47it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.77it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 10.99it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.18it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.25it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.01it/s]
Perplexity: 42.335
Average Loss: 3.746
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.375
sst2: 0.560
mrpc: 0.564
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_glue.json
{"event": "result", "adapter": "checkpoint-70", "benchmark": "perplexity", "result": {"perplexity": 42.335332018635796, "average_loss": 3.745622009714622, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-70", "benchmark": "glue", "result": {"average_score": 0.3745454545454545, "task_results": {"sst2": {"score": 0.56, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5636363636363636, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.09s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.42it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.87it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.30it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.65it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.85it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.85it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.62it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.29it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.77it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.14it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.41it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.54it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.76it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.80it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.95it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.93it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.41it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.44it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.51it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.22it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.00it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.57it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.37it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.79it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.19it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.28it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.52it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.21it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.44it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.80it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.77it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.41it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.85it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.36it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.75it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.95it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.23it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.60it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.61it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.05it/s]
Perplexity: 13.748
Average Loss: 2.621
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.513
sst2: 0.800
mrpc: 0.738
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep1_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 13.747949102082663, "average_loss": 2.620889656775516, "total_tokens": 11644}}
{"event": "result", "adapter": "ep1_bs2_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.512568306010929, "task_results": {"sst2": {"score": 0.8, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7377049180327869, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-7", "path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10/checkpoint-7"}
{"event": "benchmark_start", "adapter": "checkpoint-7", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10/checkpoint-7
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-7_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:01,  1.83s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:01<01:21,  1.20it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:34,  2.75it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:21,  4.33it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:15,  5.82it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  7.22it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.43it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:02<00:09,  9.39it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08, 10.15it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.79it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.23it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.58it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.82it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:03<00:06, 11.94it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.14it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.11it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.25it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.11it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:04<00:05, 11.46it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:04<00:05, 11.28it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.61it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.32it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.54it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:05<00:05,  9.58it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.28it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.14it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.76it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.52it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:06<00:03, 10.96it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:06<00:03, 11.41it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.47it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.67it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.06it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.43it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:07<00:02, 11.70it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.98it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.59it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.05it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:08<00:01, 10.64it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:08<00:01, 11.09it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.54it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.14it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.46it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:09<00:00, 11.45it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.55it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.62it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.76it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.76it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.44it/s]
Perplexity: 13.748
Average Loss: 2.621
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-7_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-7", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10/checkpoint-7
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-7_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.513
sst2: 0.800
mrpc: 0.738
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-7_glue.json
{"event": "result", "adapter": "checkpoint-7", "benchmark": "perplexity", "result": {"perplexity": 13.747949102082663, "average_loss": 2.620889656775516, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-7", "benchmark": "glue", "result": {"average_score": 0.512568306010929, "task_results": {"sst2": {"score": 0.8, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7377049180327869, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:21,  2.04s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.51it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.00it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.47it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.87it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.08it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.08it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.88it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.55it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.05it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.42it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.68it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.81it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.02it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.06it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.20it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.11it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.45it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.58it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.24it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.29it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.52it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.27it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.11it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.71it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.30it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.39it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.61it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.01it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.40it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.66it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.88it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.46it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.92it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.01it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.48it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.36it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.71it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.71it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.22it/s]
Perplexity: 13.818
Average Loss: 2.626
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep1_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.29it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.32it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.45it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.88it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.60it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.470
sst2: 0.710
mrpc: 0.701
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep1_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep1_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 13.818451144154698, "average_loss": 2.6260047385533705, "total_tokens": 11644}}
{"event": "result", "adapter": "ep1_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.4702849002849003, "task_results": {"sst2": {"score": 0.71, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7008547008547008, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-7", "path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA/checkpoint-7"}
{"event": "benchmark_start", "adapter": "checkpoint-7", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA/checkpoint-7
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-7_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:20,  2.03s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.52it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.02it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.49it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.89it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.12it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.11it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.91it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.60it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.10it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.44it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.67it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.80it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.02it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.06it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.19it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.09it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.43it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.25it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.53it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.20it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.26it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.54it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.09it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.65it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.44it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.50it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03,  9.73it/s]
Calculating perplexity:  64%|██████▍   | 64/100 [00:07<00:03,  9.74it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03,  9.68it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.47it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.39it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.19it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.57it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.26it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.59it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.18it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.62it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.75it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.90it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.02it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.00it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.05it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.13it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.18it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 13.818
Average Loss: 2.626
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-7_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-7", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA/checkpoint-7
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-7_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.470
sst2: 0.710
mrpc: 0.701
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-7_glue.json
{"event": "result", "adapter": "checkpoint-7", "benchmark": "perplexity", "result": {"perplexity": 13.818451144154698, "average_loss": 2.6260047385533705, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-7", "benchmark": "glue", "result": {"average_score": 0.4702849002849003, "task_results": {"sst2": {"score": 0.71, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7008547008547008, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.30it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.31it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.43it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.86it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.58it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:13,  1.96s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:27,  1.12it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:36,  2.60it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.11it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.54it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.90it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.07it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.03it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.77it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.33it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.62it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.93it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.15it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.28it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.38it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.51it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.58it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.80it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.81it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.95it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.38it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.48it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.20it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.96it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.27it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.69it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.07it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.81it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.12it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.37it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.67it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.75it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.41it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.81it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.24it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.71it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.94it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.27it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.45it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.55it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.69it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.70it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.12it/s]
Perplexity: 25.760
Average Loss: 3.249
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.440
sst2: 0.790
mrpc: 0.531
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep20_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 25.76017765783671, "average_loss": 3.248829797867523, "total_tokens": 11644}}
{"event": "result", "adapter": "ep20_bs2_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.4402040816326531, "task_results": {"sst2": {"score": 0.79, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5306122448979592, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-140", "path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10/checkpoint-140"}
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:19,  2.01s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:29,  1.09it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.53it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.01it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.42it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.68it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.87it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.89it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.65it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.31it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.81it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.14it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.29it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.53it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.81it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.86it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.03it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.98it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.31it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.48it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.18it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.24it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.54it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.06it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.64it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.43it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.28it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.36it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.59it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.93it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.33it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.54it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.81it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.90it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.56it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.05it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.63it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.77it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.04it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01,  9.92it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01,  9.98it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.57it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.83it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.11it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.56it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.63it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]
Perplexity: 25.760
Average Loss: 3.249
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.440
sst2: 0.790
mrpc: 0.531
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_glue.json
{"event": "result", "adapter": "checkpoint-140", "benchmark": "perplexity", "result": {"perplexity": 25.76017765783671, "average_loss": 3.248829797867523, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-140", "benchmark": "glue", "result": {"average_score": 0.4402040816326531, "task_results": {"sst2": {"score": 0.79, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5306122448979592, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:14,  1.97s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:27,  1.12it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.59it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.10it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.57it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  6.96it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.15it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.60it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  8.99it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.32it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:08,  9.63it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.30it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:07, 10.80it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.16it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.65it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.86it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.89it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.37it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.44it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.51it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.99it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.58it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.38it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.54it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.93it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.29it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.45it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.69it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.80it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.45it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.53it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.99it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.58it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.69it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.70it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.12it/s]
Perplexity: 17.143
Average Loss: 2.842
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.373
sst2: 0.610
mrpc: 0.510
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep20_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 17.14328395817336, "average_loss": 2.8416064909769063, "total_tokens": 11644}}
{"event": "result", "adapter": "ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.3732679738562091, "task_results": {"sst2": {"score": 0.61, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5098039215686274, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-140", "path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140"}
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:20,  2.02s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.35it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.83it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.72it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.46it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.01it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.41it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.78it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.11it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.24it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.38it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.44it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.44it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.70it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.66it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.88it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.77it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.82it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.23it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.39it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.13it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.94it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.30it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.10it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.42it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.72it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.17it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.43it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.78it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.11it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.41it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.47it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.14it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.48it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.14it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.50it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.03it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.41it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.67it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.79it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.83it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.87it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.00it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.18it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.23it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.88it/s]
Perplexity: 17.143
Average Loss: 2.842
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.373
sst2: 0.610
mrpc: 0.510
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_glue.json
{"event": "result", "adapter": "checkpoint-140", "benchmark": "perplexity", "result": {"perplexity": 17.14328395817336, "average_loss": 2.8416064909769063, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-140", "benchmark": "glue", "result": {"average_score": 0.3732679738562091, "task_results": {"sst2": {"score": 0.61, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5098039215686274, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:23,  2.06s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.96it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.40it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.76it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.95it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.92it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.68it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.35it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.82it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.16it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.38it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.12it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 10.75it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 10.27it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 10.20it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06,  9.85it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:06, 10.44it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.56it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.02it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.86it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.32it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.44it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.16it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.96it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.59it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.40it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.25it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.33it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.55it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.25it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.53it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.80it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.47it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.92it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.53it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.98it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.48it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.31it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.45it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.52it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.58it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.62it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.01it/s]
Perplexity: 16.796
Average Loss: 2.821
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.49it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.424
sst2: 0.850
mrpc: 0.422
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep5_bs1_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 16.79625589195167, "average_loss": 2.8211559979971312, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs1_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.42407407407407405, "task_results": {"sst2": {"score": 0.85, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.4222222222222222, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-65", "path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10/checkpoint-65"}
{"event": "benchmark_start", "adapter": "checkpoint-65", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10/checkpoint-65
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-65_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.49it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:21,  2.03s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.52it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.02it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.49it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.90it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.10it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.12it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.93it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.63it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.12it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.51it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.71it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.83it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.03it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.89it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.99it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.98it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.39it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.58it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.25it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.31it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.54it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.57it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.27it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.12it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.77it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.58it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 11.00it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.42it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.49it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.70it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.09it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.47it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.74it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.96it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 11.00it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.57it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.97it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.05it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.09it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.41it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.54it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.62it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.73it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.72it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.24it/s]
Perplexity: 16.796
Average Loss: 2.821
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-65_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-65", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10/checkpoint-65
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-65_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.424
sst2: 0.850
mrpc: 0.422
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-65_glue.json
{"event": "result", "adapter": "checkpoint-65", "benchmark": "perplexity", "result": {"perplexity": 16.79625589195167, "average_loss": 2.8211559979971312, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-65", "benchmark": "glue", "result": {"average_score": 0.42407407407407405, "task_results": {"sst2": {"score": 0.85, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.4222222222222222, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:29,  2.12s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.43it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.86it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.32it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.73it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.97it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.96it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.79it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.46it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.01it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.41it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.71it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.81it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:05, 12.05it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.08it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.19it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.07it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.42it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.24it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.56it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.22it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.54it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.12it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.74it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.52it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.95it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.37it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.38it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.62it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.43it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.71it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.94it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.99it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.60it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.00it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.61it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.09it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.54it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.14it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.41it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.46it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.45it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.46it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.53it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.14it/s]
Perplexity: 25.577
Average Loss: 3.242
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs1_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.233
sst2: 0.520
mrpc: 0.179
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs1_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs1_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 25.57662706275589, "average_loss": 3.241678929081163, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs1_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.23316239316239318, "task_results": {"sst2": {"score": 0.52, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.1794871794871795, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-65", "path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA/checkpoint-65"}
{"event": "benchmark_start", "adapter": "checkpoint-65", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA/checkpoint-65
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-65_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:31,  2.13s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.03it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.41it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.85it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.27it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.64it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.85it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.77it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.46it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.16it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.64it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.95it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.16it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.33it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.43it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.75it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.77it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.32it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.36it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.46it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.19it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.60it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.43it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.30it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.41it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.63it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.43it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.70it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.92it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 11.02it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.63it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.03it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.13it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.70it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.75it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.05it/s]
Perplexity: 25.577
Average Loss: 3.242
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-65_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-65", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA/checkpoint-65
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-65_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.233
sst2: 0.520
mrpc: 0.179
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-65_glue.json
{"event": "result", "adapter": "checkpoint-65", "benchmark": "perplexity", "result": {"perplexity": 25.57662706275589, "average_loss": 3.241678929081163, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-65", "benchmark": "glue", "result": {"average_score": 0.23316239316239318, "task_results": {"sst2": {"score": 0.52, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.1794871794871795, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:28,  2.10s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.45it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.88it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.31it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.89it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.89it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.70it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.39it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.89it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.28it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.58it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.74it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.96it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.90it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.09it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.02it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.40it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.57it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.24it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.28it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.53it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.55it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.90it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.30it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.12it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.39it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.67it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.80it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.22it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.70it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.15it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.46it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.74it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.83it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.49it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.94it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.01it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.07it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.52it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.61it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.73it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.70it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.08it/s]
Perplexity: 18.209
Average Loss: 2.902
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-2_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 18.209053493190563, "average_loss": 2.9019189150789604, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-2_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:04,  1.87s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:24,  1.16it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:35,  2.67it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.19it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.64it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  7.03it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.15it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.03it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.68it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.28it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.67it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.11it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.40it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.00it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 10.80it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06,  9.87it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:06,  9.70it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:06,  9.83it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.26it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.35it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.58it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.08it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.30it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.06it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.83it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.25it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.68it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.11it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.10it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.36it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.76it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.10it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.28it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.56it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.68it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.37it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.78it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.43it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.80it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.30it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.72it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.97it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.05it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.12it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.30it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 18.209
Average Loss: 2.902
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 18.209053493190563, "average_loss": 2.9019189150789604, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:17,  2.00s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:29,  1.10it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.55it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.05it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.51it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.91it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.15it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.09it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.82it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.47it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.85it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.12it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.33it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.47it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.55it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.66it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.68it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.44it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.22it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.47it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.26it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.06it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.65it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.42it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.31it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.38it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.60it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.00it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.35it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.58it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.81it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.48it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.92it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.41it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.75it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.06it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.12it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.27it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.37it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.48it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.47it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.15it/s]
Perplexity: 16.809
Average Loss: 2.822
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-2_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.326
sst2: 0.630
mrpc: 0.348
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-2_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-2_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 16.809258183611675, "average_loss": 2.821929816976932, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-2_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.3259420289855072, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.34782608695652173, "predictions": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:17,  1.99s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:28,  1.10it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.57it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.08it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.55it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  6.95it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.13it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.11it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.90it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.59it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.99it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.34it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.61it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.76it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.93it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.90it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.10it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.93it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:04<00:05, 11.28it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.47it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.65it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05,  9.96it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.22it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.00it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.91it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.22it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.68it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.12it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.42it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.75it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.10it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.37it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.81it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.42it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.83it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.43it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.35it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.66it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.23it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.29it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.62it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.62it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.15it/s]
Perplexity: 16.809
Average Loss: 2.822
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.326
sst2: 0.630
mrpc: 0.348
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 16.809258183611675, "average_loss": 2.821929816976932, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.3259420289855072, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.34782608695652173, "predictions": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:35,  2.17s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:36,  1.01it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:40,  2.37it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.79it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.18it/s]
Calculating perplexity:  10%|█         | 10/100 [00:03<00:13,  6.54it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.71it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.74it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.55it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.18it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.75it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:06, 11.14it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.40it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.70it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.74it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.85it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.87it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.25it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.10it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.32it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.39it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.47it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.18it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.89it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.29it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.64it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.01it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.17it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.46it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.79it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.05it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.34it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.64it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.73it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.38it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.83it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.45it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.36it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.77it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.96it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.24it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.22it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.30it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.95it/s]
Perplexity: 13.887
Average Loss: 2.631
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.23it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.439
sst2: 0.650
mrpc: 0.667
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r11_layers10", "benchmark": "perplexity", "result": {"perplexity": 13.886538680405195, "average_loss": 2.6309199306367805, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r11_layers10", "benchmark": "glue", "result": {"average_score": 0.4388888888888889, "task_results": {"sst2": {"score": 0.65, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6666666666666666, "predictions": [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.38it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.77it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.94it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.88it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.62it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.32it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.86it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.29it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.56it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.80it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.74it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.96it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.94it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.21it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.03it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.31it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.03it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.38it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.45it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.17it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.97it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.60it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.40it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.81it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.50it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.22it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.40it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.69it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.45it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.90it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.38it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.77it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.08it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.22it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.51it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.07it/s]
Perplexity: 13.887
Average Loss: 2.631
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.439
sst2: 0.650
mrpc: 0.667
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 13.886538680405195, "average_loss": 2.6309199306367805, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.4388888888888889, "task_results": {"sst2": {"score": 0.65, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6666666666666666, "predictions": [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:21,  2.04s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.97it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.40it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.79it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  8.00it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.98it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.80it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.48it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.94it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.27it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.53it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.68it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.89it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.90it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.01it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.93it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.28it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.42it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.40it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.47it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.19it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.03it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.64it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.41it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.75it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.22it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.23it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.45it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.77it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.43it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.90it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.00it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.83it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.23it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.24it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.43it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.61it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.13it/s]
Perplexity: 14.082
Average Loss: 2.645
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r11_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.460
sst2: 0.670
mrpc: 0.710
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r11_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r11_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.082205540557325, "average_loss": 2.6449119819680753, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r11_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.45989247311827963, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7096774193548387, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:26,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.99it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.41it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.80it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.99it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.97it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.73it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.42it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.88it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.22it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.52it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.64it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.74it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.79it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.82it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.08it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.43it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.41it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.48it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.19it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.03it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.62it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.42it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.25it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.32it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.56it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.96it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.35it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.59it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.82it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.48it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.28it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.70it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.16it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.22it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.56it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.59it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.11it/s]
Perplexity: 14.082
Average Loss: 2.645
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.460
sst2: 0.670
mrpc: 0.710
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.082205540557325, "average_loss": 2.6449119819680753, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.45989247311827963, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7096774193548387, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.18it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.28it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.32it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.59it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:07,  1.89s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:25,  1.15it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:36,  2.66it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.17it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.62it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.90it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.05it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.94it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.68it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.18it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.34it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07,  9.87it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:07,  9.77it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:07,  9.92it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 10.44it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 10.65it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 11.01it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.25it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.73it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.77it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.91it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.77it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.84it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.19it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.31it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.05it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:05,  9.75it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.24it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.04it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.47it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.86it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.94it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.27it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.62it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.00it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.20it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.49it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.54it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.15it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.42it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.06it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.41it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01,  9.95it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.27it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.51it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.75it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.86it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.98it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.06it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.15it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.04it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.91it/s]
Perplexity: 14.237
Average Loss: 2.656
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.22it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.30it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r5_layers10", "benchmark": "perplexity", "result": {"perplexity": 14.236779637631185, "average_loss": 2.655828731365525, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r5_layers10", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.30it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.05s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.49it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.97it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.41it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.79it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.98it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.96it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.72it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.32it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.80it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.06it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.30it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.44it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.61it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.68it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.68it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.89it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.29it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.38it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.11it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.87it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.51it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.31it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.74it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.19it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.31it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.46it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.79it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.11it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.28it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.60it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.73it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.29it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.59it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.28it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.76it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.28it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.67it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.83it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.14it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.15it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.31it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.43it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.03it/s]
Perplexity: 14.237
Average Loss: 2.656
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.236779637631185, "average_loss": 2.655828731365525, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:33,  2.16s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:35,  1.02it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.40it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.86it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.30it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.94it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.97it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.81it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.50it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.01it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.32it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.75it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.98it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 12.01it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.94it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.88it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.45it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.38it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.43it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.13it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.96it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.60it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.37it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.35it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.53it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.90it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.14it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.36it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.65it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.78it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.44it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.81it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.45it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.42it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.74it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.94it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.27it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.45it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.64it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.68it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.06it/s]
Perplexity: 14.770
Average Loss: 2.693
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r5_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.318
sst2: 0.690
mrpc: 0.265
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r5_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r5_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.769927750565763, "average_loss": 2.6925932048973205, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r5_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.31835341365461844, "task_results": {"sst2": {"score": 0.69, "predictions": [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.26506024096385544, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.45it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.60it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:19,  2.02s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.36it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.68it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.81it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.71it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.44it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.11it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.50it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.80it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 10.96it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.00it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 10.98it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.21it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 11.20it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.33it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.68it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.64it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.66it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.53it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.61it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.05it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.23it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  8.98it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.90it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.54it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.30it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.61it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.91it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.12it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.45it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.59it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.18it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.35it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:09<00:02, 10.11it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.43it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.08it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.05it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.82it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.12it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.15it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.30it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.36it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.88it/s]
Perplexity: 14.770
Average Loss: 2.693
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.32it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.45it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.60it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.318
sst2: 0.690
mrpc: 0.265
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.769927750565763, "average_loss": 2.6925932048973205, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.31835341365461844, "task_results": {"sst2": {"score": 0.69, "predictions": [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.26506024096385544, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:30,  2.13s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.03it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.42it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.86it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.26it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.63it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.80it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.79it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.53it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.20it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.65it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.89it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.08it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.19it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.35it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.60it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.50it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:06, 10.25it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.37it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.72it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.66it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:06<00:05, 10.82it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.21it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.37it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.13it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.87it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.42it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.27it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.71it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.12it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.24it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.50it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.91it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.17it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.39it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.80it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.47it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.91it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.49it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.94it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.42it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.83it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.99it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.30it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.52it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.65it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.67it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.95it/s]
Perplexity: 13.698
Average Loss: 2.617
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.49it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.285
sst2: 0.550
mrpc: 0.306
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 13.697966353738558, "average_loss": 2.617247380482622, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.28529411764705886, "task_results": {"sst2": {"score": 0.55, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.3058823529411765, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.89it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.32it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.92it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.93it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.75it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.42it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.88it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.25it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.54it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.67it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.92it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.97it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.11it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.04it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.35it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.45it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.51it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.22it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.95it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.58it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.38it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.80it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.17it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.44it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.19it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 10.95it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.05it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02,  9.93it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02,  9.78it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.36it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.13it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.65it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.23it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.66it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.15it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.23it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.48it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.42it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.01it/s]
Perplexity: 13.698
Average Loss: 2.617
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.285
sst2: 0.550
mrpc: 0.306
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 13.697966353738558, "average_loss": 2.617247380482622, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.28529411764705886, "task_results": {"sst2": {"score": 0.55, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.3058823529411765, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:28,  2.10s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.45it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.90it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.29it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.61it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.80it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.83it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.66it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.33it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.75it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.10it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.41it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.82it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.77it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.94it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.86it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.21it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.31it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.38it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.48it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.20it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.02it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.63it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.42it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.78it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.21it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.26it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.48it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.72it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.92it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.17it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.50it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.60it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.32it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.74it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.40it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.74it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.22it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.67it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.90it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.25it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.62it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.65it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.03it/s]
Perplexity: 14.420
Average Loss: 2.669
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.360
sst2: 0.430
mrpc: 0.650
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.420219105955562, "average_loss": 2.668631326329131, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.35985754985754986, "task_results": {"sst2": {"score": 0.43, "predictions": [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6495726495726496, "predictions": [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:23,  2.05s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.99it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.46it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.85it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.07it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.06it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.85it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.52it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.02it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.14it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.26it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.29it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.32it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.34it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.58it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.92it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.27it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.03it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.42it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.50it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.22it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.04it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.54it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.37it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.79it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.32it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.54it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.93it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.29it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.53it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.80it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.44it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.50it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.92it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.41it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.81it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.03it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.27it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.37it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.56it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.63it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]
Perplexity: 14.420
Average Loss: 2.669
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.21it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.30it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.60it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.360
sst2: 0.430
mrpc: 0.650
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.420219105955562, "average_loss": 2.668631326329131, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.35985754985754986, "task_results": {"sst2": {"score": 0.43, "predictions": [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6495726495726496, "predictions": [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.93it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.37it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.86it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.80it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.54it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.17it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.64it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.91it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.26it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.38it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.61it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.74it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.96it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.93it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.31it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.52it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.20it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.29it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.52it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.27it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.13it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.78it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.53it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.92it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.38it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.49it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.68it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.91it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.19it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.38it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.72it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.82it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.47it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.96it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.57it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.07it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.54it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.90it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.11it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.54it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.64it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.77it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.72it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.12it/s]
Perplexity: 13.325
Average Loss: 2.590
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.20it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.26it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.29it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.41it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.83it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.55it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.256
sst2: 0.530
mrpc: 0.238
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers15", "benchmark": "perplexity", "result": {"perplexity": 13.324718523645593, "average_loss": 2.5896208458997587, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers15", "benchmark": "glue", "result": {"average_score": 0.25603174603174605, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.23809523809523808, "predictions": [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:34,  2.16s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:35,  1.02it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:40,  2.39it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.79it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.22it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.62it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.87it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.61it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  9.33it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.07it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.65it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:07, 11.10it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.46it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.58it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.57it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.77it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.83it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.02it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.38it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.20it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.46it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.52it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.23it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.96it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.26it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.76it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.25it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.36it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.59it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.99it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.41it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.69it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.92it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.80it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.46it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.85it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.27it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.71it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.41it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.39it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.68it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.79it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 10.89it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.02it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.09it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.93it/s]
Perplexity: 13.325
Average Loss: 2.590
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.256
sst2: 0.530
mrpc: 0.238
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 13.324718523645593, "average_loss": 2.5896208458997587, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.25603174603174605, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.23809523809523808, "predictions": [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:26,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.46it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.89it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.30it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.63it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.80it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.69it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.35it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.00it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.47it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.82it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.09it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.26it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.40it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.42it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.61it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.66it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.86it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.89it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.98it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.32it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.44it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.16it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.89it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.38it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.19it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.62it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.08it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.10it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.21it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.47it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.75it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.81it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.44it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.86it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.49it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.75it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.22it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.63it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.84it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.17it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.25it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.70it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 10.29it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 10.25it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 10.63it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.89it/s]
Perplexity: 14.438
Average Loss: 2.670
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers15_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.385
sst2: 0.590
mrpc: 0.566
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers15_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers15_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.437627904941905, "average_loss": 2.669837847459525, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers15_genQA", "benchmark": "glue", "result": {"average_score": 0.38534591194968554, "task_results": {"sst2": {"score": 0.59, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5660377358490566, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:23,  2.05s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.49it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.97it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.42it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.81it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.03it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.02it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.78it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.41it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.79it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.18it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.41it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.58it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.82it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.85it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.98it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.95it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.30it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.46it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.22it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.47it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.53it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.24it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.00it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.60it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.82it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.26it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.36it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.58it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.97it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.32it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.58it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.83it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.88it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.50it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.40it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.75it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.95it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.18it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.11it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.21it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.49it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.12it/s]
Perplexity: 14.438
Average Loss: 2.670
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.385
sst2: 0.590
mrpc: 0.566
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.437627904941905, "average_loss": 2.669837847459525, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.38534591194968554, "task_results": {"sst2": {"score": 0.59, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5660377358490566, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:05,  1.87s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:01<01:22,  1.19it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:35,  2.73it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:21,  4.27it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.72it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  7.10it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.28it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.23it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.50it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.71it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:08,  9.90it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.25it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:07, 10.80it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.19it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.47it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.64it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.88it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.89it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:04<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.86it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.01it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.34it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.45it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.18it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.05it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.67it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.45it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:06<00:03, 10.89it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.30it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.30it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.48it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.72it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.06it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.35it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.68it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.48it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.92it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.39it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.63it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.81it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.99it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.16it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.29it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.42it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.17it/s]
Perplexity: 13.627
Average Loss: 2.612
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.289
sst2: 0.810
mrpc: 0.056
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers5", "benchmark": "perplexity", "result": {"perplexity": 13.626934577616838, "average_loss": 2.612048317818083, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers5", "benchmark": "glue", "result": {"average_score": 0.28851851851851856, "task_results": {"sst2": {"score": 0.81, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.05555555555555555, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   5%|▌         | 5/100 [00:02<00:29,  3.20it/s]
Calculating perplexity:   7%|▋         | 7/100 [00:02<00:19,  4.85it/s]
Calculating perplexity:   9%|▉         | 9/100 [00:02<00:14,  6.36it/s]
Calculating perplexity:  11%|█         | 11/100 [00:03<00:11,  7.63it/s]
Calculating perplexity:  13%|█▎        | 13/100 [00:03<00:10,  8.61it/s]
Calculating perplexity:  15%|█▌        | 15/100 [00:03<00:09,  9.44it/s]
Calculating perplexity:  17%|█▋        | 17/100 [00:03<00:08, 10.12it/s]
Calculating perplexity:  19%|█▉        | 19/100 [00:03<00:07, 10.58it/s]
Calculating perplexity:  21%|██        | 21/100 [00:03<00:07, 10.89it/s]
Calculating perplexity:  23%|██▎       | 23/100 [00:04<00:06, 11.12it/s]
Calculating perplexity:  25%|██▌       | 25/100 [00:04<00:06, 11.18it/s]
Calculating perplexity:  27%|██▋       | 27/100 [00:04<00:06, 11.44it/s]
Calculating perplexity:  29%|██▉       | 29/100 [00:04<00:06, 11.64it/s]
Calculating perplexity:  31%|███       | 31/100 [00:04<00:05, 11.75it/s]
Calculating perplexity:  33%|███▎      | 33/100 [00:04<00:05, 11.88it/s]
Calculating perplexity:  35%|███▌      | 35/100 [00:05<00:05, 10.98it/s]
Calculating perplexity:  37%|███▋      | 37/100 [00:05<00:05, 11.21it/s]
Calculating perplexity:  39%|███▉      | 39/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  41%|████      | 41/100 [00:05<00:05, 11.27it/s]
Calculating perplexity:  43%|████▎     | 43/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  45%|████▌     | 45/100 [00:06<00:05, 10.61it/s]
Calculating perplexity:  47%|████▋     | 47/100 [00:06<00:05, 10.10it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.29it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.00it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.55it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.37it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.72it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.07it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.45it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.81it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.20it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.48it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.74it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.83it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.50it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.95it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.53it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.00it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.72it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.94it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.21it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.44it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.54it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.67it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.67it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.05it/s]
Perplexity: 13.627
Average Loss: 2.612
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.49it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.93it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.289
sst2: 0.810
mrpc: 0.056
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 13.626934577616838, "average_loss": 2.612048317818083, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.28851851851851856, "task_results": {"sst2": {"score": 0.81, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.05555555555555555, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:29,  2.12s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.48it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.96it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.42it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.80it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.03it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.04it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.86it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.52it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.93it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.27it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.56it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.69it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.77it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.81it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.99it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.89it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.20it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.39it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.18it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.35it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.09it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.86it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.52it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.36it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.34it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.39it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.56it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.97it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.40it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.69it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.91it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.91it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.50it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.93it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.53it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.98it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.44it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.83it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.30it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.30it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.09it/s]
Perplexity: 14.404
Average Loss: 2.668
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-3_r8_layers5_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.462
sst2: 0.670
mrpc: 0.715
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-3_r8_layers5_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers5_genQA", "benchmark": "perplexity", "result": {"perplexity": 14.404262490559576, "average_loss": 2.6675241690695963, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-3_r8_layers5_genQA", "benchmark": "glue", "result": {"average_score": 0.4618157181571816, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7154471544715447, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:38,  2.20s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:37,  1.00it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:40,  2.34it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.76it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.18it/s]
Calculating perplexity:  10%|█         | 10/100 [00:03<00:13,  6.55it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.78it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.78it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.54it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.27it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.79it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:07, 11.04it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.35it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.48it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.81it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.89it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.86it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.29it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.37it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.45it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.17it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.89it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.31it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.73it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.22it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.49it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.85it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.16it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.43it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.66it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.72it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.39it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.71it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.30it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.71it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.27it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.70it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.90it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.13it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.19it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.29it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.35it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.93it/s]
Perplexity: 14.404
Average Loss: 2.668
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.462
sst2: 0.670
mrpc: 0.715
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 14.404262490559576, "average_loss": 2.6675241690695963, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.4618157181571816, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7154471544715447, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.05s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.95it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.35it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.60it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.76it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.75it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.48it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.17it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.73it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.16it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.61it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.83it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.87it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.93it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.92it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.25it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.44it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.23it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.48it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.56it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.26it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.10it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.61it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.40it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.21it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.49it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.17it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.36it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.69it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.37it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.43it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.93it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.43it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.22it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.27it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.55it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.44it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.07it/s]
Perplexity: 13.311
Average Loss: 2.589
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.473
sst2: 0.730
mrpc: 0.690
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-4_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 13.310845585820744, "average_loss": 2.5885791605033237, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-4_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.4732183908045977, "task_results": {"sst2": {"score": 0.73, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6896551724137931, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:26,  2.09s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.46it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.38it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.76it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.97it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.98it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.72it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.37it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.84it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.06it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.32it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.48it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.70it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.76it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.92it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.91it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.42it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.18it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.45it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.51it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.23it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.52it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.30it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.73it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.14it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.16it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.08it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.32it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.63it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.74it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.38it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.34it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.79it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.29it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.63it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.18it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.41it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 13.311
Average Loss: 2.589
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.473
sst2: 0.730
mrpc: 0.690
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 13.310845585820744, "average_loss": 2.5885791605033237, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.4732183908045977, "task_results": {"sst2": {"score": 0.73, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6896551724137931, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:05,  1.87s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:01<01:22,  1.18it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:35,  2.72it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.25it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.67it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  7.04it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.18it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.15it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.85it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.48it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.93it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.21it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.38it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.46it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.71it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.77it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:04<00:05, 11.15it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.34it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.08it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.42it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.49it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.21it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:05,  9.80it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.28it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.09it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:06<00:04, 10.48it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.85it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.95it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.31it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.63it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.02it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.39it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.30it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.40it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.38it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.71it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.76it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.02it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.08it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.04it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.40it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.17it/s]
Perplexity: 12.399
Average Loss: 2.518
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs2_lr1e-4_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.335
sst2: 0.720
mrpc: 0.286
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs2_lr1e-4_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs2_lr1e-4_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 12.39894089567179, "average_loss": 2.5176110573238377, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs2_lr1e-4_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.3352380952380953, "task_results": {"sst2": {"score": 0.72, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.2857142857142857, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-35", "path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA/checkpoint-35"}
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:19,  2.02s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.09it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:37,  2.53it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.03it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.48it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.83it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.03it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.03it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.83it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.51it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.00it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.27it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:06, 11.51it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.65it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.84it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.86it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.98it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.94it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.32it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.48it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.24it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.54it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.09it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.69it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.47it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.90it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.33it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.41it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.62it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.02it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.37it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.62it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.44it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.88it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.33it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.76it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.97it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.29it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.43it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.65it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.63it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.18it/s]
Perplexity: 12.399
Average Loss: 2.518
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-35", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA/checkpoint-35
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-35_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.335
sst2: 0.720
mrpc: 0.286
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-35_glue.json
{"event": "result", "adapter": "checkpoint-35", "benchmark": "perplexity", "result": {"perplexity": 12.39894089567179, "average_loss": 2.5176110573238377, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-35", "benchmark": "glue", "result": {"average_score": 0.3352380952380953, "task_results": {"sst2": {"score": 0.72, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.2857142857142857, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10", "path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10"}
{"event": "benchmark_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:29,  2.12s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.41it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.82it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.22it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.51it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.60it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:10,  8.57it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  9.27it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.96it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.46it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:07, 10.61it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 10.96it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.03it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.18it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.32it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 11.30it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.47it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.83it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.83it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.11it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.92it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:06<00:05, 10.91it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.28it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.42it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.16it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:05,  9.61it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.14it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.09it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.58it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.44it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.87it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.23it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.49it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:09<00:02, 10.34it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.80it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.45it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.74it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.17it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.64it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.80it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.09it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.21it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:11<00:00, 11.29it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.32it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.88it/s]
Perplexity: 11.993
Average Loss: 2.484
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.200
sst2: 0.600
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_glue.json
{"event": "result", "adapter": "ep5_bs4_lr1e-3_r8_layers10", "benchmark": "perplexity", "result": {"perplexity": 11.993341290179698, "average_loss": 2.484351603293165, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs4_lr1e-3_r8_layers10", "benchmark": "glue", "result": {"average_score": 0.19999999999999998, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-20", "path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10/checkpoint-20"}
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:30,  2.13s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.42it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.87it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.31it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.70it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.76it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.80it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.63it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.36it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.88it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.28it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.57it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.72it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.63it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.79it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.84it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.02it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.31it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.44it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.51it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.23it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.92it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.54it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.38it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.82it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.26it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.35it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.54it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.78it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.12it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.18it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.57it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.71it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.41it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.90it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.45it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.38it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.81it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.01it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.31it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.60it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.62it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 11.993
Average Loss: 2.484
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.200
sst2: 0.600
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_glue.json
{"event": "result", "adapter": "checkpoint-20", "benchmark": "perplexity", "result": {"perplexity": 11.993341290179698, "average_loss": 2.484351603293165, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-20", "benchmark": "glue", "result": {"average_score": 0.19999999999999998, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.09s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.46it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.91it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.31it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.69it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.89it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.89it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.70it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.38it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.88it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.21it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.51it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.63it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.81it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.85it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.89it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.87it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.16it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.00it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.08it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.40it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.49it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.21it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.90it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.49it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.30it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.78it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.24it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.28it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.57it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.99it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.38it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.64it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.88it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.95it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.55it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 11.00it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.60it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.47it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.02it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.48it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.74it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.75it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]
Perplexity: 11.015
Average Loss: 2.399
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "ep5_bs4_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.236
sst2: 0.600
mrpc: 0.108
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/ep5_bs4_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "ep5_bs4_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 11.014817372905041, "average_loss": 2.3992414002625733, "total_tokens": 11644}}
{"event": "result", "adapter": "ep5_bs4_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.23603603603603604, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.10810810810810811, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-20", "path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA/checkpoint-20"}
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.93it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.35it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.70it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.87it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.86it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.63it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.27it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.75it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.07it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.36it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.52it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.70it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.69it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.75it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.00it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.30it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.10it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.39it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.48it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.20it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.51it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.32it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.69it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.04it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.11it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.33it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.74it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.11it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.28it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.62it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.66it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.34it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.57it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.21it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.15it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.56it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.70it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.82it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.83it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 10.78it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 10.82it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.04it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.15it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.96it/s]
Perplexity: 11.015
Average Loss: 2.399
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.236
sst2: 0.600
mrpc: 0.108
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_glue.json
{"event": "result", "adapter": "checkpoint-20", "benchmark": "perplexity", "result": {"perplexity": 11.014817372905041, "average_loss": 2.3992414002625733, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-20", "benchmark": "glue", "result": {"average_score": 0.23603603603603604, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.10810810810810811, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.04s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.97it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.39it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.71it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.80it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.67it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  9.30it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.92it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.40it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.66it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 10.97it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.07it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.25it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.31it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.49it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.64it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.79it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.80it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.95it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.81it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.91it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.26it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.39it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.13it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.81it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.28it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.10it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.54it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.99it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.17it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.47it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.26it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.53it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.77it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.51it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.92it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.54it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.00it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.47it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.05it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.58it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.53it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.00it/s]
Perplexity: 11.954
Average Loss: 2.481
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.167
sst2: 0.500
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 11.95392422034099, "average_loss": 2.481059611109842, "total_tokens": 11644}}
{"event": "result", "adapter": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.16666666666666666, "task_results": {"sst2": {"score": 0.5, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-20", "path": "/workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20"}
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:23,  2.06s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.07it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.48it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.91it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.32it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.70it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.91it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.79it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.44it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.01it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.36it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.85it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.06it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.22it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.40it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.47it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.54it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.59it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.94it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.87it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.01it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.35it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.45it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.18it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.85it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.34it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.12it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.49it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.80it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.88it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.22it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.54it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.76it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.29it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.39it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.16it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.65it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.34it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.84it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.37it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.79it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 11.01it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.39it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.49it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.60it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.61it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.96it/s]
Perplexity: 11.954
Average Loss: 2.481
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.167
sst2: 0.500
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_glue.json
{"event": "result", "adapter": "checkpoint-20", "benchmark": "perplexity", "result": {"perplexity": 11.95392422034099, "average_loss": 2.481059611109842, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-20", "benchmark": "glue", "result": {"average_score": 0.16666666666666666, "task_results": {"sst2": {"score": 0.5, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.10s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.45it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.93it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.38it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.78it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.01it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.03it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.84it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.51it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.93it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.22it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.31it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.34it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.46it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.62it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.79it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.82it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.92it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.09it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.91it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.94it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.30it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.44it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.17it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.05it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.69it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.91it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.35it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.42it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.39it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.31it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.47it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 10.92it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.42it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.58it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.32it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.83it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.48it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.96it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.46it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.05it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.33it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.48it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.58it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.67it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.65it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 23.141
Average Loss: 3.142
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 23.14128750868637, "average_loss": 3.1416183601782026, "total_tokens": 11644}}
{"event": "result", "adapter": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-140", "path": "/workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-140"}
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.04s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.50it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.93it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.35it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.71it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.87it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.84it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.61it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.26it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.77it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.18it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.63it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.86it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.91it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.08it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:05, 11.01it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.30it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.44it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.22it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.48it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.53it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.24it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.07it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.62it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.39it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.83it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.26it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.36it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.56it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.94it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.26it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.28it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.57it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.68it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.38it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.48it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.84it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.32it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.66it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.86it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.10it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.19it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.51it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]
Perplexity: 23.141
Average Loss: 3.142
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_glue.json
{"event": "result", "adapter": "checkpoint-140", "benchmark": "perplexity", "result": {"perplexity": 23.14128750868637, "average_loss": 3.1416183601782026, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-140", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.10s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.45it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.91it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.31it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.68it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.56it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:10,  8.17it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  8.75it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.63it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.31it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:07, 10.83it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.22it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.47it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.82it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.87it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.88it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.24it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.41it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.21it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.47it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.53it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.24it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.97it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.46it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.29it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.66it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.12it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.54it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.96it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.34it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.61it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.51it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.91it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.53it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.64it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.11it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.56it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.82it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.99it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.11it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.30it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.43it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.57it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.61it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.99it/s]
Perplexity: 23.424
Average Loss: 3.154
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 23.423581762113056, "average_loss": 3.153743282518449, "total_tokens": 11644}}
{"event": "result", "adapter": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-40", "path": "/workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40"}
{"event": "benchmark_start", "adapter": "checkpoint-40", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-40_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:30,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.52it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.00it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.44it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.81it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.01it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.98it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.72it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.31it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.82it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.19it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.68it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.90it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.91it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.99it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.88it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.22it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.10it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.41it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.19it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.46it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.52it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.23it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.03it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.58it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.34it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.75it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.20it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.31it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.50it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.85it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.20it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.45it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.70it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.06it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.14it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02,  9.78it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.34it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.01it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.36it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.52it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.72it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.86it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.04it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.07it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.01it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.01it/s]
Perplexity: 23.424
Average Loss: 3.154
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-40_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-40", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-40_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.160
sst2: 0.480
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-40_glue.json
{"event": "result", "adapter": "checkpoint-40", "benchmark": "perplexity", "result": {"perplexity": 23.423581762113056, "average_loss": 3.153743282518449, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-40", "benchmark": "glue", "result": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:27,  2.09s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.05it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.45it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.90it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.33it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.70it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.91it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.90it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.61it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.28it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.59it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.87it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.16it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.34it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.60it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.75it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.90it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.86it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.46it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.08it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.38it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.47it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.21it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.45it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.29it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.74it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.19it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.29it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.49it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.88it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.25it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.49it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.77it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.87it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.48it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.94it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.33it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.65it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.72it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.99it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.12it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.32it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.50it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.69it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.70it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.05it/s]
Perplexity: 12.931
Average Loss: 2.560
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.229
sst2: 0.630
mrpc: 0.056
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 12.931093685506506, "average_loss": 2.5596347743192926, "total_tokens": 11644}}
{"event": "result", "adapter": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.2287793427230047, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.056338028169014086, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-20", "path": "/workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20"}
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:22,  2.05s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:31,  1.08it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.51it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.99it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.45it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.84it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.05it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.73it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  9.08it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.37it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.03it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.41it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:07, 10.79it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.15it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.48it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.52it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.81it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.83it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.25it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.12it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.40it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.22it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.48it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.53it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.24it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.06it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.64it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.40it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.18it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.35it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.47it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.71it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.11it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.54it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.66it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.32it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.79it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.44it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.82it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.32it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.68it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.87it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.99it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.12it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.17it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.17it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.98it/s]
Perplexity: 12.931
Average Loss: 2.560
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-20", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-20_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.229
sst2: 0.630
mrpc: 0.056
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-20_glue.json
{"event": "result", "adapter": "checkpoint-20", "benchmark": "perplexity", "result": {"perplexity": 12.931093685506506, "average_loss": 2.5596347743192926, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-20", "benchmark": "glue", "result": {"average_score": 0.2287793427230047, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.056338028169014086, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "path": "/workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA"}
{"event": "benchmark_start", "adapter": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:28,  2.11s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:33,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.44it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.88it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.30it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.65it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.86it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.87it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.56it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.21it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.74it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.18it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.42it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.49it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.73it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.67it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.89it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.89it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.04it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.20it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.98it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.37it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.46it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.18it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.00it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.42it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.24it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.59it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.97it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.05it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.37it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.74it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.11it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.34it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.67it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.79it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.47it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.84it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.46it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.92it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.31it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.76it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.02it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.16it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.18it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.32it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.98it/s]
Perplexity: 52.240
Average Loss: 3.956
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA_perplexity.json
{"event": "benchmark_start", "adapter": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.384
sst2: 0.530
mrpc: 0.621
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA_glue.json
{"event": "result", "adapter": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "perplexity", "result": {"perplexity": 52.239790172678255, "average_loss": 3.9558444683574723, "total_tokens": 11644}}
{"event": "result", "adapter": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "benchmark": "glue", "result": {"average_score": 0.3837864077669903, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6213592233009708, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-140", "path": "/workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140"}
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:29,  2.12s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:34,  1.04it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:39,  2.42it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:24,  3.84it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.26it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.56it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.67it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:10,  8.58it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.43it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.17it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.55it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.91it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.00it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.13it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.36it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.45it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.64it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.66it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.82it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.74it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 10.97it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.83it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.98it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.32it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.43it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.16it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.93it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.45it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.21it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.67it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.93it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.11it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.32it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.76it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.98it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.19it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.49it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.55it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:09<00:02, 10.20it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.52it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.20it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.04it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.38it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.55it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.82it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 10.94it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.04it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.15it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:11<00:00, 11.24it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.37it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.87it/s]
Perplexity: 52.240
Average Loss: 3.956
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-140", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-140_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.384
sst2: 0.530
mrpc: 0.621
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-140_glue.json
{"event": "result", "adapter": "checkpoint-140", "benchmark": "perplexity", "result": {"perplexity": 52.239790172678255, "average_loss": 3.9558444683574723, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-140", "benchmark": "glue", "result": {"average_score": 0.3837864077669903, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6213592233009708, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   5%|▌         | 5/100 [00:02<00:29,  3.21it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  4.04it/s]
Calculating perplexity:   7%|▋         | 7/100 [00:02<00:18,  4.92it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:15,  5.78it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  7.30it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.65it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:08,  9.64it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08, 10.30it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.86it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 11.26it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:06, 11.25it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.66it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.88it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.93it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 12.10it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:05, 11.03it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.26it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.47it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.17it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.23it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.49it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.53it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.09it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.70it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.48it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.85it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.19it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.22it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.43it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.87it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.21it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.47it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.75it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.49it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.97it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.58it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.51it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.88it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.06it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.37it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.51it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.71it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.71it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 18.364
Average Loss: 2.910
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.463
sst2: 0.680
mrpc: 0.708
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 18.363590500695814, "average_loss": 2.910369927112854, "total_tokens": 11644}}
{"event": "result", "adapter": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.4626548672566371, "task_results": {"sst2": {"score": 0.68, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7079646017699115, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-40", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40"}
{"event": "benchmark_start", "adapter": "checkpoint-40", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-40_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.33it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:24,  2.07s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.48it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.96it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.43it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.82it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:10,  8.03it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.99it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.75it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.46it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.98it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:06, 11.35it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.50it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.53it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.76it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.70it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.87it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.87it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.23it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.44it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.15it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:04, 11.21it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.48it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.54it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.25it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.10it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.67it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.44it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.34it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.42it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.64it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 11.02it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.25it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.34it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.60it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.64it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.36it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.85it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.47it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.89it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.32it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.68it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.82it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:01, 10.98it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.42it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.43it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.09it/s]
Perplexity: 18.364
Average Loss: 2.910
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-40_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-40", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-40_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.28it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.36it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.92it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.463
sst2: 0.680
mrpc: 0.708
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-40_glue.json
{"event": "result", "adapter": "checkpoint-40", "benchmark": "perplexity", "result": {"perplexity": 18.363590500695814, "average_loss": 2.910369927112854, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-40", "benchmark": "glue", "result": {"average_score": 0.4626548672566371, "task_results": {"sst2": {"score": 0.68, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7079646017699115, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:25,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.94it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.34it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.63it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.69it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:10,  8.54it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:09,  9.22it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08,  9.79it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.21it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:04<00:07, 10.33it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:07, 10.77it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 10.96it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.19it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.29it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.35it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:05<00:06, 10.48it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.98it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.94it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.10it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.92it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.02it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.35it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.45it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.17it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.88it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.37it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.22it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.63it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.82it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 10.89it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.18it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:08<00:03, 10.48it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:03, 10.73it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.03it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.49it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.60it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:09<00:02, 10.23it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.56it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.22it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.01it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01,  9.49it/s]
Calculating perplexity:  84%|████████▍ | 84/100 [00:09<00:01,  9.54it/s]
Calculating perplexity:  86%|████████▌ | 86/100 [00:10<00:01,  9.98it/s]
Calculating perplexity:  88%|████████▊ | 88/100 [00:10<00:01, 10.54it/s]
Calculating perplexity:  90%|█████████ | 90/100 [00:10<00:00, 10.85it/s]
Calculating perplexity:  92%|█████████▏| 92/100 [00:10<00:00, 11.03it/s]
Calculating perplexity:  94%|█████████▍| 94/100 [00:10<00:00, 11.08it/s]
Calculating perplexity:  96%|█████████▌| 96/100 [00:10<00:00, 11.11it/s]
Calculating perplexity:  98%|█████████▊| 98/100 [00:11<00:00, 11.20it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00, 11.43it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.82it/s]
Perplexity: 24.296
Average Loss: 3.190
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.23it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.31it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.33it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.90it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.163
sst2: 0.490
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 24.296204710111294, "average_loss": 3.190320153379309, "total_tokens": 11644}}
{"event": "result", "adapter": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.16333333333333333, "task_results": {"sst2": {"score": 0.49, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-70", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-70"}
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:09,  1.92s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:25,  1.14it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:36,  2.63it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.11it/s]
Calculating perplexity:   7%|▋         | 7/100 [00:02<00:19,  4.82it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.53it/s]
Calculating perplexity:   9%|▉         | 9/100 [00:02<00:14,  6.24it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  6.97it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  8.00it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.04it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.81it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.41it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.84it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.12it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.33it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.39it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.43it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 11.24it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 11.32it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.44it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:06, 10.63it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.66it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.06it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.88it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.88it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.26it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.41it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.15it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04, 10.02it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.54it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04, 10.37it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.84it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.27it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.37it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.49it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.81it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.01it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.27it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.62it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.76it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.44it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.93it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:01, 10.55it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 11.03it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.48it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.91it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 11.10it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.38it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.37it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.37it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.40it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.59it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.62it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]
Perplexity: 24.296
Average Loss: 3.190
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-70", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-70
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-70_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.163
sst2: 0.490
mrpc: 0.000
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-70_glue.json
{"event": "result", "adapter": "checkpoint-70", "benchmark": "perplexity", "result": {"perplexity": 24.296204710111294, "average_loss": 3.190320153379309, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-70", "benchmark": "glue", "result": {"average_score": 0.16333333333333333, "task_results": {"sst2": {"score": 0.49, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA"}
{"event": "benchmark_start", "adapter": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:01<03:05,  1.87s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:24,  1.16it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:36,  2.66it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:22,  4.18it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:16,  5.62it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:12,  6.95it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:02<00:10,  8.09it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  9.03it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.73it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:07, 10.36it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.76it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 10.77it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:03<00:07, 10.78it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:07, 10.52it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 10.55it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:06, 10.86it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:06, 11.24it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.48it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 10.85it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 10.82it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.14it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 10.94it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 10.99it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:05<00:05, 10.34it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.43it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.16it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:04,  9.94it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04, 10.57it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:06<00:04, 10.35it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:03, 10.78it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 11.23it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.33it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.59it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.99it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:07<00:02, 11.36it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.63it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.87it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.84it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.48it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:08<00:02, 10.86it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.46it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.90it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.35it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.65it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:09<00:01, 10.81it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:09<00:00, 11.09it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.20it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.34it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.35it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.26it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:10<00:00, 11.25it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:10<00:00,  9.12it/s]
Perplexity: 13.209
Average Loss: 2.581
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA_perplexity.json
{"event": "benchmark_start", "adapter": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.24it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.515
sst2: 0.800
mrpc: 0.744
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA_glue.json
{"event": "result", "adapter": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "perplexity", "result": {"perplexity": 13.208622509151123, "average_loss": 2.580869836726986, "total_tokens": 11644}}
{"event": "result", "adapter": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "benchmark": "glue", "result": {"average_score": 0.5146005509641873, "task_results": {"sst2": {"score": 0.8, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.743801652892562, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "adapter_start", "adapter": "checkpoint-10", "path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-10"}
{"event": "benchmark_start", "adapter": "checkpoint-10", "benchmark": "perplexity"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-10_perplexity.json
benchmark: perplexity
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.25it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.34it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.61it/s]

=== Running Perplexity Benchmark ===
Warning: Penn Treebank not available (Dataset scripts are no longer supported, but found ptb_text_only.py) — falling back to WikiText-2 raw.
Loaded fallback dataset 'wikitext-2-raw-v1' split='validation'
Calculating perplexity on 100 texts...

Calculating perplexity:   0%|          | 0/100 [00:00<?, ?it/s]
Calculating perplexity:   1%|          | 1/100 [00:02<03:26,  2.08s/it]
Calculating perplexity:   2%|▏         | 2/100 [00:02<01:32,  1.06it/s]
Calculating perplexity:   4%|▍         | 4/100 [00:02<00:38,  2.47it/s]
Calculating perplexity:   6%|▌         | 6/100 [00:02<00:23,  3.95it/s]
Calculating perplexity:   8%|▊         | 8/100 [00:02<00:17,  5.39it/s]
Calculating perplexity:  10%|█         | 10/100 [00:02<00:13,  6.77it/s]
Calculating perplexity:  12%|█▏        | 12/100 [00:03<00:11,  7.97it/s]
Calculating perplexity:  14%|█▍        | 14/100 [00:03<00:09,  8.91it/s]
Calculating perplexity:  16%|█▌        | 16/100 [00:03<00:08,  9.67it/s]
Calculating perplexity:  18%|█▊        | 18/100 [00:03<00:08, 10.20it/s]
Calculating perplexity:  20%|██        | 20/100 [00:03<00:07, 10.68it/s]
Calculating perplexity:  22%|██▏       | 22/100 [00:03<00:07, 11.00it/s]
Calculating perplexity:  24%|██▍       | 24/100 [00:04<00:06, 11.25it/s]
Calculating perplexity:  26%|██▌       | 26/100 [00:04<00:06, 11.32it/s]
Calculating perplexity:  28%|██▊       | 28/100 [00:04<00:06, 11.59it/s]
Calculating perplexity:  30%|███       | 30/100 [00:04<00:05, 11.69it/s]
Calculating perplexity:  32%|███▏      | 32/100 [00:04<00:05, 11.82it/s]
Calculating perplexity:  34%|███▍      | 34/100 [00:04<00:06, 10.85it/s]
Calculating perplexity:  36%|███▌      | 36/100 [00:05<00:05, 11.13it/s]
Calculating perplexity:  38%|███▊      | 38/100 [00:05<00:05, 11.05it/s]
Calculating perplexity:  40%|████      | 40/100 [00:05<00:05, 11.33it/s]
Calculating perplexity:  42%|████▏     | 42/100 [00:05<00:05, 11.07it/s]
Calculating perplexity:  44%|████▍     | 44/100 [00:05<00:05, 11.10it/s]
Calculating perplexity:  46%|████▌     | 46/100 [00:06<00:05, 10.40it/s]
Calculating perplexity:  48%|████▊     | 48/100 [00:06<00:05,  9.49it/s]
Calculating perplexity:  49%|████▉     | 49/100 [00:06<00:05,  9.21it/s]
Calculating perplexity:  51%|█████     | 51/100 [00:06<00:05,  9.49it/s]
Calculating perplexity:  53%|█████▎    | 53/100 [00:06<00:04,  9.59it/s]
Calculating perplexity:  55%|█████▌    | 55/100 [00:07<00:04,  9.60it/s]
Calculating perplexity:  57%|█████▋    | 57/100 [00:07<00:04, 10.15it/s]
Calculating perplexity:  59%|█████▉    | 59/100 [00:07<00:03, 10.76it/s]
Calculating perplexity:  61%|██████    | 61/100 [00:07<00:03, 11.01it/s]
Calculating perplexity:  63%|██████▎   | 63/100 [00:07<00:03, 10.36it/s]
Calculating perplexity:  65%|██████▌   | 65/100 [00:07<00:03, 10.74it/s]
Calculating perplexity:  67%|██████▋   | 67/100 [00:08<00:02, 11.03it/s]
Calculating perplexity:  69%|██████▉   | 69/100 [00:08<00:02, 11.35it/s]
Calculating perplexity:  71%|███████   | 71/100 [00:08<00:02, 10.69it/s]
Calculating perplexity:  73%|███████▎  | 73/100 [00:08<00:02, 10.74it/s]
Calculating perplexity:  75%|███████▌  | 75/100 [00:08<00:02, 10.40it/s]
Calculating perplexity:  77%|███████▋  | 77/100 [00:09<00:02, 10.78it/s]
Calculating perplexity:  79%|███████▉  | 79/100 [00:09<00:02, 10.41it/s]
Calculating perplexity:  81%|████████  | 81/100 [00:09<00:01, 10.77it/s]
Calculating perplexity:  83%|████████▎ | 83/100 [00:09<00:01, 10.24it/s]
Calculating perplexity:  85%|████████▌ | 85/100 [00:09<00:01, 10.59it/s]
Calculating perplexity:  87%|████████▋ | 87/100 [00:10<00:01, 10.85it/s]
Calculating perplexity:  89%|████████▉ | 89/100 [00:10<00:00, 11.21it/s]
Calculating perplexity:  91%|█████████ | 91/100 [00:10<00:00, 11.28it/s]
Calculating perplexity:  93%|█████████▎| 93/100 [00:10<00:00, 11.36it/s]
Calculating perplexity:  95%|█████████▌| 95/100 [00:10<00:00, 11.41it/s]
Calculating perplexity:  97%|█████████▋| 97/100 [00:10<00:00, 11.56it/s]
Calculating perplexity:  99%|█████████▉| 99/100 [00:11<00:00, 11.60it/s]
Calculating perplexity: 100%|██████████| 100/100 [00:11<00:00,  8.99it/s]
Perplexity: 13.209
Average Loss: 2.581
Total Tokens: 11644

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-10_perplexity.json
{"event": "benchmark_start", "adapter": "checkpoint-10", "benchmark": "glue"}
`torch_dtype` is deprecated! Use `dtype` instead!
Starting benchmark...
model_name: Qwen/Qwen3-8B
adapter_path: /workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-10
needle_size: 2048
needle_type: qa_1
max_samples: 100
output_file: /workspace/NLP_Project/outputs/checkpoint-10_glue.json
benchmark: glue
perplexity_texts: None
perplexity_split: validation
mmlu_subset: all
glue_task: all
batch_size: 8
Using data file: needles/2048/qa_1_2048.json
Loading model...

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.26it/s]
Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.32it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]
Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.91it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.62it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

=== Running GLUE Benchmark ===
Running GLUE task: sst2
Running GLUE task: mrpc
Running GLUE task: qnli
Error in GLUE task qnli: 'premise'
Average GLUE Score: 0.515
sst2: 0.800
mrpc: 0.744
qnli: 0.000

Detailed results saved to /workspace/NLP_Project/outputs/checkpoint-10_glue.json
{"event": "result", "adapter": "checkpoint-10", "benchmark": "perplexity", "result": {"perplexity": 13.208622509151123, "average_loss": 2.580869836726986, "total_tokens": 11644}}
{"event": "result", "adapter": "checkpoint-10", "benchmark": "glue", "result": {"average_score": 0.5146005509641873, "task_results": {"sst2": {"score": 0.8, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.743801652892562, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}
{"event": "summary", "results": {"ep0_bs2_lr1e-3_r8_layers10": {"adapter_name": "ep0_bs2_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10", "perplexity": {"perplexity": 14.273432405025368, "average_loss": 2.6583999355033434, "total_tokens": 11644}, "glue": {"average_score": 0.4826558265582655, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7479674796747967, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep0_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep0_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep0_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 14.273432405025368, "average_loss": 2.6583999355033434, "total_tokens": 11644}, "glue": {"average_score": 0.4826558265582655, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7479674796747967, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep10_bs2_lr1e-3_r8_layers10": {"adapter_name": "ep10_bs2_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10", "perplexity": {"perplexity": 20.0425938574836, "average_loss": 2.9978597018469735, "total_tokens": 11644}, "glue": {"average_score": 0.31666666666666665, "task_results": {"sst2": {"score": 0.7, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.25, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-70": {"adapter_name": "checkpoint-70", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-70", "perplexity": {"perplexity": 24.296204710111294, "average_loss": 3.190320153379309, "total_tokens": 11644}, "glue": {"average_score": 0.16333333333333333, "task_results": {"sst2": {"score": 0.49, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep10_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep10_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep10_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 42.335332018635796, "average_loss": 3.745622009714622, "total_tokens": 11644}, "glue": {"average_score": 0.3745454545454545, "task_results": {"sst2": {"score": 0.56, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5636363636363636, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep1_bs2_lr1e-3_r8_layers10": {"adapter_name": "ep1_bs2_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10", "perplexity": {"perplexity": 13.747949102082663, "average_loss": 2.620889656775516, "total_tokens": 11644}, "glue": {"average_score": 0.512568306010929, "task_results": {"sst2": {"score": 0.8, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7377049180327869, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-7": {"adapter_name": "checkpoint-7", "adapter_path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA/checkpoint-7", "perplexity": {"perplexity": 13.818451144154698, "average_loss": 2.6260047385533705, "total_tokens": 11644}, "glue": {"average_score": 0.4702849002849003, "task_results": {"sst2": {"score": 0.71, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7008547008547008, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep1_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep1_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep1_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 13.818451144154698, "average_loss": 2.6260047385533705, "total_tokens": 11644}, "glue": {"average_score": 0.4702849002849003, "task_results": {"sst2": {"score": 0.71, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7008547008547008, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep20_bs2_lr1e-3_r8_layers10": {"adapter_name": "ep20_bs2_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10", "perplexity": {"perplexity": 25.76017765783671, "average_loss": 3.248829797867523, "total_tokens": 11644}, "glue": {"average_score": 0.4402040816326531, "task_results": {"sst2": {"score": 0.79, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5306122448979592, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-140": {"adapter_name": "checkpoint-140", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA/checkpoint-140", "perplexity": {"perplexity": 52.239790172678255, "average_loss": 3.9558444683574723, "total_tokens": 11644}, "glue": {"average_score": 0.3837864077669903, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6213592233009708, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep20_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep20_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep20_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 17.14328395817336, "average_loss": 2.8416064909769063, "total_tokens": 11644}, "glue": {"average_score": 0.3732679738562091, "task_results": {"sst2": {"score": 0.61, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5098039215686274, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs1_lr1e-3_r8_layers10": {"adapter_name": "ep5_bs1_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10", "perplexity": {"perplexity": 16.79625589195167, "average_loss": 2.8211559979971312, "total_tokens": 11644}, "glue": {"average_score": 0.42407407407407405, "task_results": {"sst2": {"score": 0.85, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.4222222222222222, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-65": {"adapter_name": "checkpoint-65", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA/checkpoint-65", "perplexity": {"perplexity": 25.57662706275589, "average_loss": 3.241678929081163, "total_tokens": 11644}, "glue": {"average_score": 0.23316239316239318, "task_results": {"sst2": {"score": 0.52, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.1794871794871795, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs1_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep5_bs1_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs1_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 25.57662706275589, "average_loss": 3.241678929081163, "total_tokens": 11644}, "glue": {"average_score": 0.23316239316239318, "task_results": {"sst2": {"score": 0.52, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.1794871794871795, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-2_r8_layers10": {"adapter_name": "ep5_bs2_lr1e-2_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10", "perplexity": {"perplexity": 18.209053493190563, "average_loss": 2.9019189150789604, "total_tokens": 11644}, "glue": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-35": {"adapter_name": "checkpoint-35", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA/checkpoint-35", "perplexity": {"perplexity": 12.39894089567179, "average_loss": 2.5176110573238377, "total_tokens": 11644}, "glue": {"average_score": 0.3352380952380953, "task_results": {"sst2": {"score": 0.72, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.2857142857142857, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-2_r8_layers10_genQA": {"adapter_name": "ep5_bs2_lr1e-2_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-2_r8_layers10_genQA", "perplexity": {"perplexity": 16.809258183611675, "average_loss": 2.821929816976932, "total_tokens": 11644}, "glue": {"average_score": 0.3259420289855072, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.34782608695652173, "predictions": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r11_layers10": {"adapter_name": "ep5_bs2_lr1e-3_r11_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10", "perplexity": {"perplexity": 13.886538680405195, "average_loss": 2.6309199306367805, "total_tokens": 11644}, "glue": {"average_score": 0.4388888888888889, "task_results": {"sst2": {"score": 0.65, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6666666666666666, "predictions": [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r11_layers10_genQA": {"adapter_name": "ep5_bs2_lr1e-3_r11_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r11_layers10_genQA", "perplexity": {"perplexity": 14.082205540557325, "average_loss": 2.6449119819680753, "total_tokens": 11644}, "glue": {"average_score": 0.45989247311827963, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7096774193548387, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r5_layers10": {"adapter_name": "ep5_bs2_lr1e-3_r5_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10", "perplexity": {"perplexity": 14.236779637631185, "average_loss": 2.655828731365525, "total_tokens": 11644}, "glue": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r5_layers10_genQA": {"adapter_name": "ep5_bs2_lr1e-3_r5_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r5_layers10_genQA", "perplexity": {"perplexity": 14.769927750565763, "average_loss": 2.6925932048973205, "total_tokens": 11644}, "glue": {"average_score": 0.31835341365461844, "task_results": {"sst2": {"score": 0.69, "predictions": [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.26506024096385544, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers10": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10", "perplexity": {"perplexity": 13.697966353738558, "average_loss": 2.617247380482622, "total_tokens": 11644}, "glue": {"average_score": 0.28529411764705886, "task_results": {"sst2": {"score": 0.55, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.3058823529411765, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 14.420219105955562, "average_loss": 2.668631326329131, "total_tokens": 11644}, "glue": {"average_score": 0.35985754985754986, "task_results": {"sst2": {"score": 0.43, "predictions": [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6495726495726496, "predictions": [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers15": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers15", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15", "perplexity": {"perplexity": 13.324718523645593, "average_loss": 2.5896208458997587, "total_tokens": 11644}, "glue": {"average_score": 0.25603174603174605, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.23809523809523808, "predictions": [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers15_genQA": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers15_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers15_genQA", "perplexity": {"perplexity": 14.437627904941905, "average_loss": 2.669837847459525, "total_tokens": 11644}, "glue": {"average_score": 0.38534591194968554, "task_results": {"sst2": {"score": 0.59, "predictions": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.5660377358490566, "predictions": [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers5": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers5", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5", "perplexity": {"perplexity": 13.626934577616838, "average_loss": 2.612048317818083, "total_tokens": 11644}, "glue": {"average_score": 0.28851851851851856, "task_results": {"sst2": {"score": 0.81, "predictions": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.05555555555555555, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-3_r8_layers5_genQA": {"adapter_name": "ep5_bs2_lr1e-3_r8_layers5_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-3_r8_layers5_genQA", "perplexity": {"perplexity": 14.404262490559576, "average_loss": 2.6675241690695963, "total_tokens": 11644}, "glue": {"average_score": 0.4618157181571816, "task_results": {"sst2": {"score": 0.67, "predictions": [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7154471544715447, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-4_r8_layers10": {"adapter_name": "ep5_bs2_lr1e-4_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10", "perplexity": {"perplexity": 13.310845585820744, "average_loss": 2.5885791605033237, "total_tokens": 11644}, "glue": {"average_score": 0.4732183908045977, "task_results": {"sst2": {"score": 0.73, "predictions": [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6896551724137931, "predictions": [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs2_lr1e-4_r8_layers10_genQA": {"adapter_name": "ep5_bs2_lr1e-4_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs2_lr1e-4_r8_layers10_genQA", "perplexity": {"perplexity": 12.39894089567179, "average_loss": 2.5176110573238377, "total_tokens": 11644}, "glue": {"average_score": 0.3352380952380953, "task_results": {"sst2": {"score": 0.72, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.2857142857142857, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs4_lr1e-3_r8_layers10": {"adapter_name": "ep5_bs4_lr1e-3_r8_layers10", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10", "perplexity": {"perplexity": 11.993341290179698, "average_loss": 2.484351603293165, "total_tokens": 11644}, "glue": {"average_score": 0.19999999999999998, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-20": {"adapter_name": "checkpoint-20", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-20", "perplexity": {"perplexity": 12.931093685506506, "average_loss": 2.5596347743192926, "total_tokens": 11644}, "glue": {"average_score": 0.2287793427230047, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.056338028169014086, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "ep5_bs4_lr1e-3_r8_layers10_genQA": {"adapter_name": "ep5_bs4_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/ep5_bs4_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 11.014817372905041, "average_loss": 2.3992414002625733, "total_tokens": 11644}, "glue": {"average_score": 0.23603603603603604, "task_results": {"sst2": {"score": 0.6, "predictions": [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.10810810810810811, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/niah_single_2_131072/niah_single_2_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 11.95392422034099, "average_loss": 2.481059611109842, "total_tokens": 11644}, "glue": {"average_score": 0.16666666666666666, "task_results": {"sst2": {"score": 0.5, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/niah_single_2_2048/niah_single_2_2048_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 23.14128750868637, "average_loss": 3.1416183601782026, "total_tokens": 11644}, "glue": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/niah_single_2_32768/niah_single_2_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 23.423581762113056, "average_loss": 3.153743282518449, "total_tokens": 11644}, "glue": {"average_score": 0.16, "task_results": {"sst2": {"score": 0.48, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-40": {"adapter_name": "checkpoint-40", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-40", "perplexity": {"perplexity": 18.363590500695814, "average_loss": 2.910369927112854, "total_tokens": 11644}, "glue": {"average_score": 0.4626548672566371, "task_results": {"sst2": {"score": 0.68, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7079646017699115, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_131072/qa_1_131072_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 12.931093685506506, "average_loss": 2.5596347743192926, "total_tokens": 11644}, "glue": {"average_score": 0.2287793427230047, "task_results": {"sst2": {"score": 0.63, "predictions": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.056338028169014086, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA": {"adapter_name": "qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_2048/qa_1_2048_ep20_bs2_lr1e-3_r8_layers10_genQA", "perplexity": {"perplexity": 52.239790172678255, "average_loss": 3.9558444683574723, "total_tokens": 11644}, "glue": {"average_score": 0.3837864077669903, "task_results": {"sst2": {"score": 0.53, "predictions": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.6213592233009708, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep20_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 18.363590500695814, "average_loss": 2.910369927112854, "total_tokens": 11644}, "glue": {"average_score": 0.4626548672566371, "task_results": {"sst2": {"score": 0.68, "predictions": [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.7079646017699115, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep35_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 24.296204710111294, "average_loss": 3.190320153379309, "total_tokens": 11644}, "glue": {"average_score": 0.16333333333333333, "task_results": {"sst2": {"score": 0.49, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.0, "predictions": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA": {"adapter_name": "qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA", "perplexity": {"perplexity": 13.208622509151123, "average_loss": 2.580869836726986, "total_tokens": 11644}, "glue": {"average_score": 0.5146005509641873, "task_results": {"sst2": {"score": 0.8, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.743801652892562, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}, "checkpoint-10": {"adapter_name": "checkpoint-10", "adapter_path": "/workspace/NLP_Project/adapters/qa_1_32768/qa_1_32768_ep5_bs2_lr1e-3_r8_layers10_noGenQA/checkpoint-10", "perplexity": {"perplexity": 13.208622509151123, "average_loss": 2.580869836726986, "total_tokens": 11644}, "glue": {"average_score": 0.5146005509641873, "task_results": {"sst2": {"score": 0.8, "predictions": [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], "labels": [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0]}, "mrpc": {"score": 0.743801652892562, "predictions": [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], "labels": [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]}, "qnli": {"score": 0, "predictions": [], "labels": []}}}}}}
{"event": "done"}
